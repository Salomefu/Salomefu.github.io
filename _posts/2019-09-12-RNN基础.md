---
layout: post
title: RNN基础
subtitle: 
date: 2019-09-12
author: Salome
header-img: img/post-bg-2015.jpg
catalog: true
tags:
    - machine learning
---



# 循环序列模型（Recurrent Neural Networks）



[TOC]



#  1. Intuition



主要是吴恩达deeplearning.ai课程notes，在一些没有讲清楚的地方我会加一些解释，以及分别从top-down和bottom-up的角度看待RNN架构及其变体。

## 从简单的前馈神经网络到RNN发生了什么变化

首先logistic是一个最简单的神经网络，没有隐藏神经元，只有输入输出。接着比较复杂的就是只有一个隐藏层的神经网络。这时候什么变了？结构。原先所有的输入特征只和1个神经元相连，现在我们将它和多个神经元相连，期待不同的神经元能学出不同的特征表达（因为每个连接权值不同）。再将学出来的向量进行下一层连接。深度前馈神经网络比起三层的并没有什么不同。比较重要的trick是**向量化技术**，通过Tensor的操作使得多个样本多个特征可以并行得出结果，而不是多个for循环。不是很理解的可以去看前面博客的代码，有详细介绍。

对于简单的前馈神经网络，至少要掌握：

1. 前向和后向的推导，包括梯度下降细节
2. 前向和后向的纯python代码实现
3. 知道如何利用向量化技术

从前馈神经网络到RNN结构发生了变化，同样是三层神经网络，但是在隐藏层加了点料。以前是直接$\sigma(wx+b)$,现在还需要加入之前的信息，变成了$\sigma(w_{1}x+b+w_{2}a_{t-1})$,就是说要加上上一个时间的隐藏层输出向量再做输出。如果理解了如何从概念上可行的东西转换成数学上可行的公式我觉得RNN的所有变体就不再难理解了。无论是LSTM或者是GRU或者ATTENTION，都是想让它加上某个信息，而这个“加”最简单的可以直接把之前时间步骤的信息拿过来，也可以做的很复杂比如让之前时间步的某几步通过某种数学公式组合起来。



# 2. Class NOTES

