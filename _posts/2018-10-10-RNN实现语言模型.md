---
layout: post
title: RNN实现语言模型
subtitle: 
date: 2018-10-10
author: Salome
header-img: img/post-bg-2015.jpg
catalog: true
tags:
    - machine learning
---

[TOC]



# 1. intuition

语言模型即P(x1,x2,x3..)出现的概率。p(x1,x2,x3) = p(x1)p(x2|x1)p(x3|x1,x2),当有训练预料时，直接统计即可计算概率值。RNN架构可以看成是在建模条件概率。当训练好模型之后，便可以用此网络建模语言生成，这里要注意的是**每个时间步我们并不是想要最大的概率值对应的词语，而是从概率分布中抽样**。



# 2 class notes

## 2.1不同类型的循环神经网络（Different types of **RNN**s）

现在你已经了解了一种**RNN**结构，它的输入量$T_{x}$等于输出数量$T_{y}$。事实上，对于其他一些应用，$T_{x}$和$T_{y}$并不一定相等。在这个视频里，你会看到更多的**RNN**的结构。

![](https://tva1.sinaimg.cn/large/006y8mN6gy1g98xkrlknrj30go09adhf.jpg)

你应该还记得这周第一个视频中的那个幻灯片，那里有很多例子输入$x$和输出$y$，有各种类型，并不是所有的情况都满足$T_{x}=T_{y}$。

比如音乐生成这个例子，$T_{x}$可以是长度为1甚至为空集。再比如电影情感分类，输出$y$可以是1到5的整数，而输入是一个序列。在命名实体识别中，这个例子中输入长度和输出长度是一样的。

还有一些情况，输入长度和输出长度不同，他们都是序列但长度不同，比如机器翻译，一个法语句子和一个英语句子不同数量的单词却能表达同一个意思。

所以我们应该修改基本的**RNN**结构来处理这些问题，这个视频的内容参考了**Andrej Karpathy**的博客，一篇叫做《循环神经网络的非理性效果》（“**The Unreasonable Effectiveness of Recurrent Neural Networks**”）的文章，我们看一些例子。

你已经见过$T_{x} = T_{y}$的例子了（下图编号1所示），也就是我们输入序列$x^{<1>}$，$x^{<2>}$，一直到$x^{< T_{x}>}$，我们的循环神经网络这样工作，输入$x^{<1>}$来计算$\hat y^{<1>}$，$\hat y^{<2>}$等等一直到$\hat y^{<T_{y}>}$。在原先的图里，我会画一串圆圈表示神经元，大部分时候为了让符号更加简单，此处就以简单的小圈表示。这个就叫做“多对多”（**many-to-many**）的结构，因为输入序列有很多的输入，而输出序列也有很多输出。

现在我们看另外一个例子，假如说，你想处理情感分类问题（下图编号2所示），这里$x$可能是一段文本，比如一个电影的评论，“**These is nothing to like in this movie.**”（“这部电影没什么还看的。”），所以$x$就是一个序列，而$y$可能是从1到5的一个数字，或者是0或1，这代表正面评价和负面评价，而数字1到5代表电影是1星，2星，3星，4星还是5星。所以在这个例子中，我们可以简化神经网络的结构，输入$x^{<1 >}$，$x^{< 2 >}$，一次输入一个单词，如果输入文本是“**These is nothing to like in this movie**”，那么单词的对应如下图编号2所示。我们不再在每个时间上都有输出了，而是让这个**RNN**网络读入整个句子，然后在最后一个时间上得到输出，这样输入的就是整个句子，所以这个神经网络叫做“多对一”（**many-to-one**）结构，因为它有很多输入，很多的单词，然后输出一个数字。

![](https://tva1.sinaimg.cn/large/006y8mN6gy1g98xkp3y3qj30go08jwf8.jpg)

为了完整性，还要补充一个“**一对一**”（**one-to-one**）的结构（上图编号3所示），这个可能没有那么重要，这就是一个小型的标准的神经网络，输入$x$然后得到输出$y$，我们这个系列课程的前两个课程已经讨论过这种类型的神经网络了。

![](https://tva1.sinaimg.cn/large/006y8mN6gy1g98xkwal7mj30go07zq3v.jpg)

除了“**多对一**”的结构，也可以有“**一对多**”（**one-to-many**）的结构。对于一个“一对多”神经网络结构的例子就是音乐生成（上图编号1所示），事实上，你会在这个课后编程练习中去实现这样的模型，你的目标是使用一个神经网络输出一些音符。对应于一段音乐，输入$x$可以是一个整数，表示你想要的音乐类型或者是你想要的音乐的第一个音符，并且如果你什么都不想输入，$x$可以是空的输入，可设为0向量。

这样这个神经网络的结构，首先是你的输入$x$，然后得到**RNN**的输出，第一个值，然后就没有输入了，再得到第二个输出，接着输出第三个值等等，一直到合成这个音乐作品的最后一个音符，这里也可以写上输入$a^{<0>}$（上图编号3所示）。有一个后面才会讲到的技术细节，当你生成序列时通常会把第一个合成的输出也喂给下一层（上图编号4所示），所以实际的网络结构最终就像这个样子。

我们已经讨论了“**多对多**”、“**多对一**”、“**一对一**”和“**一对多**”的结构，对于“多对多”的结构还有一个有趣的例子值得详细说一下，就是输入和输出长度不同的情况。你刚才看过的多对多的例子，它的输入长度和输出长度是完全一样的。而对于像机器翻译这样的应用，输入句子的单词的数量，比如说一个法语的句子，和输出句子的单词数量，比如翻译成英语，这两个句子的长度可能不同，所以还需要一个新的网络结构，一个不同的神经网络（上图编号2所示）。首先读入这个句子，读入这个输入，比如你要将法语翻译成英语，读完之后，这个网络就会输出翻译结果。有了这种结构$T_{x}$和$T_{y}$就可以是不同的长度了。同样，你也可以画上这个$a^{<0>}$。这个网络的结构有两个不同的部分，这（上图编号5所示）是一个编码器，获取输入，比如法语句子，这（上图编号6所示）是解码器，它会读取整个句子，然后输出翻译成其他语言的结果。

![](https://tva1.sinaimg.cn/large/006y8mN6gy1g98xkqk9hmj30go0930tq.jpg)

这就是一个“**多对多**”结构的例子，到这周结束的时候，你就能对这些各种各样结构的基本构件有一个很好的理解。严格来说，还有一种结构，我们会在第四周涉及到，就是“注意力”（**attention based**）结构，但是根据我们现在画的这些图不好理解这个模型。

总结一下这些各种各样的**RNN**结构，这（上图编号1所示）是“**一对一**”的结构，当去掉$a^{<0>}$时它就是一种标准类型的神经网络。还有一种“**一对多**”的结构（上图编号2所示），比如音乐生成或者序列生成。还有“**多对一**”，这（上图编号3所示）是情感分类的例子，首先读取输入，一个电影评论的文本，然后判断他们是否喜欢电影还是不喜欢。还有“**多对多**”的结构（上图编号4所示），命名实体识别就是“**多对多**”的例子，其中$T_{x}=T_{y}$。最后还有一种“**多对多**”结构的其他版本（上图编号5所示），对于像机器翻译这样的应用，$T_{x}$和$T_{y}$就可以不同了。

现在，你已经了解了大部分基本的模块，这些就是差不多所有的神经网络了，除了序列生成，有些细节的问题我们会在下节课讲解。

我希望你从本视频中了解到用这些**RNN**的基本模块，把它们组合在一起就可以构建各种各样的模型。但是正如我前面提到的，序列生成还有一些不一样的地方，在这周的练习里，你也会实现它，你需要构建一个语言模型，结果好的话会得到一些有趣的序列或者有意思的文本。下节课深入探讨序列生成。

## 2.2 语言模型和序列生成（Language model and sequence generation）

在自然语言处理中，构建语言模型是最基础的也是最重要的工作之一，并且能用**RNN**很好地实现。在本视频中，你将学习用**RNN**构建一个语言模型，在本周结束的时候，还会有一个很有趣的编程练习，你能在练习中构建一个语言模型，并用它来生成莎士比亚文风的文本或其他类型文本。

![](https://tva1.sinaimg.cn/large/006y8mN6gy1g98xkoj1uqj30go09ewfd.jpg)

所以什么是语言模型呢？比如你在做一个语音识别系统，你听到一个句子，“**the apple and pear（pair） salad was delicious.**”，所以我究竟说了什么？我说的是 “**the apple and pair salad**”，还是“**the apple and pear salad**”？（**pear**和**pair**是近音词）。你可能觉得我说的应该更像第二种，事实上，这就是一个好的语音识别系统要帮助输出的东西，即使这两句话听起来是如此相似。而让语音识别系统去选择第二个句子的方法就是使用一个语言模型，他能计算出这两句话各自的可能性。

举个例子，一个语音识别模型可能算出第一句话的概率是$P( \text{The apple  and  pair  salad}) = 3.2 \times 10^{-13}$，而第二句话的概率是$P\left(\text{The apple  and  pear salad} \right) = 5.7 \times 10^{-10}$，比较这两个概率值，显然我说的话更像是第二种，因为第二句话的概率比第一句高出1000倍以上，这就是为什么语音识别系统能够在这两句话中作出选择。

所以语言模型所做的就是，它会告诉你某个特定的句子它出现的概率是多少，根据我所说的这个概率，假设你随机拿起一张报纸，打开任意邮件，或者任意网页或者听某人说下一句话，并且这个人是你的朋友，这个你即将从世界上的某个地方得到的句子会是某个特定句子的概率是多少，例如“**the apple and pear salad**”。它是两种系统的基本组成部分，一个刚才所说的语音识别系统，还有机器翻译系统，它要能正确输出最接近的句子。而语言模型做的最基本工作就是输入一个句子，准确地说是一个文本序列，$y^{<1>}$，$y^{<2>}$一直到$y^{<T_{y}>}$。对于语言模型来说，用$y$来表示这些序列比用$x$来表示要更好，然后语言模型会估计某个句子序列中各个单词出现的可能性。

那么如何建立一个语言模型呢？为了使用**RNN**建立出这样的模型，你首先需要一个训练集，包含一个很大的英文文本语料库（**corpus**）或者其它的语言，你想用于构建模型的语言的语料库。语料库是自然语言处理的一个专有名词，意思就是很长的或者说数量众多的英文句子组成的文本。

![](https://tva1.sinaimg.cn/large/006y8mN6gy1g98xkuczfcj30go08z3zg.jpg)

假如说，你在训练集中得到这么一句话，“**Cats average 15 hours of sleep a day.**”(猫一天睡15小时)，你要做的第一件事就是将这个句子标记化，意思就是像之前视频中一样，建立一个字典，然后将每个单词都转换成对应的**one-hot**向量，也就是字典中的索引。可能还有一件事就是你要定义句子的结尾，一般的做法就是增加一个额外的标记，叫做**EOS**（上图编号1所示），它表示句子的结尾，这样能够帮助你搞清楚一个句子什么时候结束，我们之后会详细讨论这个。**EOS**标记可以被附加到训练集中每一个句子的结尾，如果你想要你的模型能够准确识别句子结尾的话。在本周的练习中我们不需要使用这个**EOS**标记，不过在某些应用中你可能会用到它，不过稍后就能见到它的用处。于是在本例中我们，如果你加了**EOS**标记，这句话就会有9个输入，有$y^{<1>}$，$y^{<2>}$一直到$y^{<9>}$。在标记化的过程中，你可以自行决定要不要把标点符号看成标记，在本例中，我们忽略了标点符号，所以我们只把**day**看成标志，不包括后面的句号，如果你想把句号或者其他符号也当作标志，那么你可以将句号也加入你的字典中。

现在还有一个问题如果你的训练集中有一些词并不在你的字典里，比如说你的字典有10,000个词，10,000个最常用的英语单词。现在这个句，“**The Egyptian Mau is a bread of cat.**”其中有一个词**Mau**，它可能并不是预先的那10,000个最常用的单词，在这种情况下，你可以把**Mau**替换成一个叫做**UNK**的代表未知词的标志，我们只针对**UNK**建立概率模型，而不是针对这个具体的词**Mau**。

完成标识化的过程后，这意味着输入的句子都映射到了各个标志上，或者说字典中的各个词上。下一步我们要构建一个**RNN**来构建这些序列的概率模型。在下一张幻灯片中会看到的一件事就是最后你会将$x^{<t>}$设为$y^{<t-1>}$。

![](https://tva1.sinaimg.cn/large/006y8mN6gy1g98xkx751gj30go09cabh.jpg)

现在我们来建立**RNN**模型，我们继续使用“**Cats average 15 hours of sleep a day.**”这个句子来作为我们的运行样例，我将会画出一个**RNN**结构。在第0个时间步，你要计算激活项$a^{<1>}$，它是以$x^{<1 >}$作为输入的函数，而$x^{<1>}$会被设为全为0的集合，也就是0向量。在之前的$a^{<0>}$按照惯例也设为0向量，于是$a^{<1>}$要做的就是它会通过**softmax**进行一些预测来计算出第一个词可能会是什么，其结果就是$\hat y^{<1>}$（上图编号1所示），这一步其实就是通过一个**softmax**层来预测字典中的任意单词会是第一个词的概率，比如说第一个词是$a$的概率有多少，第一个词是**Aaron**的概率有多少，第一个词是**cats**的概率又有多少，就这样一直到**Zulu**是第一个词的概率是多少，还有第一个词是**UNK**（未知词）的概率有多少，还有第一个词是句子结尾标志的概率有多少，表示不必阅读。所以$\hat y^{<1>}$的输出是**softmax**的计算结果，它只是预测第一个词的概率，而不去管结果是什么。在我们的例子中，最终会得到单词**Cats**。所以**softmax**层输出10,000种结果，因为你的字典中有10,000个词，或者会有10,002个结果，因为你可能加上了未知词，还有句子结尾这两个额外的标志。

然后**RNN**进入下个时间步，在下一时间步中，仍然使用激活项$a^{<1>}$，在这步要做的是计算出第二个词会是什么。现在我们依然传给它正确的第一个词，我们会告诉它第一个词就是**Cats**，也就是$\hat y^{<1>}$，告诉它第一个词就是**Cats**，这就是为什么$y^{<1>} = x^{<2>}$（上图编号2所示）。然后在第二个时间步中，输出结果同样经过**softmax**层进行预测，**RNN**的职责就是预测这些词的概率（上图编号3所示），而不会去管结果是什么，可能是b或者**arron**，可能是**Cats**或者**Zulu**或者**UNK**（未知词）或者**EOS**或者其他词，它只会考虑之前得到的词。所以在这种情况下，我猜正确答案会是**average**，因为句子确实就是**Cats average**开头的。

然后再进行**RNN**的下个时间步，现在要计算$a^{<3>}$。为了预测第三个词，也就是15，我们现在给它之前两个词，告诉它**Cats average**是句子的前两个词，所以这是下一个输入，$x^{<3>} = y^{<2>}$，输入**average**以后，现在要计算出序列中下一个词是什么，或者说计算出字典中每一个词的概率（上图编号4所示），通过之前得到的**Cats**和**average**，在这种情况下，正确结果会是15，以此类推。

一直到最后，没猜错的话，你会停在第9个时间步，然后把$x^{<9>}$也就是$y^{<8>}$传给它（上图编号5所示），也就是单词**day**，这里是$a^{<9>}$，它会输出$y^{<9>}$，最后的得到结果会是**EOS**标志，在这一步中，通过前面这些得到的单词，不管它们是什么，我们希望能预测出**EOS**句子结尾标志的概率会很高（上图编号6所示）。

所以**RNN**中的每一步都会考虑前面得到的单词，比如给它前3个单词（上图编号7所示），让它给出下个词的分布，这就是**RNN**如何学习从左往右地每次预测一个词。

接下来为了训练这个网络，我们要定义代价函数。于是，在某个时间步$t$，如果真正的词是$y^{<t>}$，而神经网络的**softmax**层预测结果值是$y^{<t>}$，那么这（上图编号8所示）就是**softmax**损失函数，$L\left( \hat y^{<t>},y^{<t>}>\right) = - \sum_{i}^{}{y_{i}^{<t>}\log\hat y_{i}^{<t>}}$。而总体损失函数（上图编号9所示）$L = \sum_{t}^{}{L^{< t >}\left( \hat y^{<t>},y^{<t>} \right)}$，也就是把所有单个预测的损失函数都相加起来。

![](https://tva1.sinaimg.cn/large/006y8mN6gy1g98xkshpauj308c04w3yo.jpg)

如果你用很大的训练集来训练这个**RNN**，你就可以通过开头一系列单词像是**Cars average 15**或者**Cars average 15 hours of**来预测之后单词的概率。现在有一个新句子，它是$y^{<1>}$，$y^{<2>}$，$y^{<3>}$，为了简单起见，它只包含3个词（如上图所示），现在要计算出整个句子中各个单词的概率，方法就是第一个**softmax**层会告诉你$y^{<1>}$的概率（上图编号1所示），这也是第一个输出，然后第二个**softmax**层会告诉你在考虑$y^{<1>}$的情况下$y^{<2>}$的概率（上图编号2所示），然后第三个**softmax**层告诉你在考虑$y^{<1>}$和$y^{<2>}$的情况下$y^{<3>}$的概率（上图编号3所示），把这三个概率相乘，最后得到这个含3个词的整个句子的概率。

这就是用**RNN**训练一个语言模型的基础结构，可能我说的这些东西听起来有些抽象，不过别担心，你可以在编程练习中亲自实现这些东西。下一节课用语言模型做的一件最有趣的事就是从模型中进行采样。

## 2.3 对新序列采样（Sampling novel sequences）

在你训练一个序列模型之后，要想了解到这个模型学到了什么，一种非正式的方法就是进行一次新序列采样，来看看到底应该怎么做。

记住一个序列模型模拟了任意特定单词序列的概率，我们要做的就是对这些概率分布进行采样来生成一个新的单词序列。下图编号1所示的网络已经被上方所展示的结构训练训练过了，而为了进行采样（下图编号2所示的网络），你要做一些截然不同的事情。

![](https://tva1.sinaimg.cn/large/006y8mN6gy1g98xkppx8yj30go099jsm.jpg)

第一步要做的就是对你想要模型生成的第一个词进行采样，于是你输入$x^{<1>} =0$，$a^{<0>} =0$，现在你的第一个时间步得到的是所有可能的输出是经过**softmax**层后得到的概率，然后根据这个**softmax**的分布进行随机采样。**Softmax**分布给你的信息就是第一个词**a**的概率是多少，第一个词是**aaron**的概率是多少，第一个词是**zulu**的概率是多少，还有第一个词是**UNK**（未知标识）的概率是多少，这个标识可能代表句子的结尾，然后对这个向量使用例如**numpy**命令，`np.random.choice`（上图编号3所示），来根据向量中这些概率的分布进行采样，这样就能对第一个词进行采样了。

然后继续下一个时间步，记住第二个时间步需要$\hat y^{<1>}$作为输入，而现在要做的是把刚刚采样得到的$\hat y^{<1>}$放到$a^{<2>}$（上图编号4所示），作为下一个时间步的输入，所以不管你在第一个时间步得到的是什么词，都要把它传递到下一个位置作为输入，然后**softmax**层就会预测$\hat y^{<2>}$是什么。举个例子，假如说对第一个词进行抽样后，得到的是**The**，**The**作为第一个词的情况很常见，然后把**The**当成$x^{<2>}$，现在$x^{<2>}$就是$\hat y^{<1>}$，现在你要计算出在第一词是**The**的情况下，第二个词应该是什么（上图编号5所示），然后得到的结果就是$\hat y^{<2>}$，然后再次用这个采样函数来对$\hat y^{<2>}$进行采样。

然后再到下一个时间步，无论你得到什么样的用**one-hot**码表示的选择结果，都把它传递到下一个时间步，然后对第三个词进行采样。不管得到什么都把它传递下去，一直这样直到最后一个时间步。

那么你要怎样知道一个句子结束了呢？方法之一就是，如果代表句子结尾的标识在你的字典中，你可以一直进行采样直到得到**EOS**标识（上图编号6所示），这代表着已经抵达结尾，可以停止采样了。另一种情况是，如果你的字典中没有这个词，你可以决定从20个或100个或其他个单词进行采样，然后一直将采样进行下去直到达到所设定的时间步。不过这种过程有时候会产生一些未知标识（上图编号7所示），如果你要确保你的算法不会输出这种标识，你能做的一件事就是拒绝采样过程中产生任何未知的标识，一旦出现就继续在剩下的词中进行重采样，直到得到一个不是未知标识的词。如果你不介意有未知标识产生的话，你也可以完全不管它们。

这就是你如何从你的**RNN**语言模型中生成一个随机选择的句子。直到现在我们所建立的是基于词汇的**RNN**模型，意思就是字典中的词都是英语单词（下图编号1所示）。

![](https://tva1.sinaimg.cn/large/006y8mN6gy1g98xktohxvj30go09cq3u.jpg)

根据你实际的应用，你还可以构建一个基于字符的**RNN**结构，在这种情况下，你的字典仅包含从**a**到**z**的字母，可能还会有空格符，如果你需要的话，还可以有数字0到9，如果你想区分字母大小写，你可以再加上大写的字母，你还可以实际地看一看训练集中可能会出现的字符，然后用这些字符组成你的字典（上图编号2所示）。

如果你建立一个基于字符的语言模型，比起基于词汇的语言模型，你的序列$\hat y^{<1>}$，$\hat y^{<2>}$，$\hat y^{<3>}$在你的训练数据中将会是单独的字符，而不是单独的词汇。所以对于前面的例子来说，那个句子（上图编号3所示），“**Cats average 15 hours of sleep a day.**”，在该例中**C**就是$\hat y^{<1>}$，**a**就是$\hat y^{<2>}$，**t**就是$\hat y^{<3>}$，空格符就是$\hat y^{<4>}$等等。

使用基于字符的语言模型有有点也有缺点，优点就是你不必担心会出现未知的标识，例如基于字符的语言模型会将**Mau**这样的序列也视为可能性非零的序列。而对于基于词汇的语言模型，如果**Mau**不在字典中，你只能把它当作未知标识**UNK**。不过基于字符的语言模型一个主要缺点就是你最后会得到太多太长的序列，大多数英语句子只有10到20个的单词，但却可能包含很多很多字符。所以基于字符的语言模型在捕捉句子中的依赖关系也就是句子较前部分如何影响较后部分不如基于词汇的语言模型那样可以捕捉长范围的关系，并且基于字符的语言模型训练起来计算成本比较高昂。所以我见到的自然语言处理的趋势就是，绝大多数都是使用基于词汇的语言模型，但随着计算机性能越来越高，会有更多的应用。在一些特殊情况下，会开始使用基于字符的模型。但是这确实需要更昂贵的计算力来训练，所以现在并没有得到广泛地使用，除了一些比较专门需要处理大量未知的文本或者未知词汇的应用，还有一些要面对很多专有词汇的应用。

在现有的方法下，现在你可以构建一个**RNN**结构，看一看英文文本的语料库，然后建立一个基于词汇的或者基于字符的语言模型，然后从训练的语言模型中进行采样。

![](https://tva1.sinaimg.cn/large/006y8mN6gy1g98xkvbfdfj30go07ct9i.jpg)

这里有一些样本，它们是从一个语言模型中采样得到的，准确来说是基于字符的语言模型，你可以在编程练习中自己实现这样的模型。如果模型是用新闻文章训练的，它就会生成左边这样的文本，这有点像一篇不太合乎语法的新闻文本，不过听起来，这句“**Concussion epidemic**”，**to be examined**，确实有点像新闻报道。用莎士比亚的文章训练后生成了右边这篇东西，听起来很像是莎士比亚写的东西：

“**The mortal moon hath her eclipse in love.**

**And subject of this thou art another this fold.**

**When besser be my love to me see sabl's.**

**For whose are ruse of mine eyes heaves.**”

这些就是基础的**RNN**结构和如何去建立一个语言模型并使用它，对于训练出的语言模型进行采样。在之后的视频中，我想探讨在训练**RNN**时一些更加深入的挑战以及如何适应这些挑战，特别是梯度消失问题来建立更加强大的**RNN**模型。下节课，我们将谈到梯度消失并且会开始谈到**GRU**，也就是门控循环单元和**LSTM**长期记忆网络模型。

# 3. Code

​	

```python
# 之后补充
```

