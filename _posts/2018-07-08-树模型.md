---
Glayout: post
title: 树模型
subtitle: 
date: 2018-07-08
author: Salome
header-img: img/post-bg-2015.jpg
catalog: true
tags:
    - machine learning
---

# 概况

从所有的决策树中选取最优决策树是NP-Hard问题，所以决策树学习算法通常采用启发式方法，近似求解这一最优化问题。决策树学习算法是一个递归地选择最优特征，并根据该特征对训练数据进行分割的过程。递归进行直至所有样本都被正确分类或者没有更多的特征为止。决策树常用的算法有ID3、C4.5、CART算法。

# ID3和C4.5

ID3通过信息增益选择特征,特征A对训练数据集D的信息增益定义为集合D的经验熵与特征A给定条件下D的经验条件熵之差，等价于类与特征的互信息。
$$
g(D,A)=H(D)-H(D|A)
$$
由于某个特征所能取得值越多，那么条件熵越大。所以用信息增益会偏向于选择取值多的特征，所以在C4.5中用信息增益比替代信息增益。
$$
g_R(D,A)=g(D,A)/H_A(D)
$$

## 剪枝

决策树的剪枝往往通过极小化决策树整体的损失函数来实现。假设树T的叶节点个数为｜T｜，t是树T的叶节点，该叶节点有$N_t$个样本点，其中k类的样本点有$N_{tk}$个，$H_t(T)$为叶节点上的经验熵，则决策函数的损失函数可以定义为
$$
C(T)=\sum_{t=1}^{|T|}N_tH_t(T)+\alpha|T|
$$
每个节点上的熵（节点样本数作为权重，样本数越多权重越大）与叶子节点的个数的和。第一项代表拟合程度，第二项代表复杂度。剪枝就是通过递归的计算每个节点上有此节点的损失函数值与无此节点的损失函数值的差来确定是否去掉该节点。

## CART

CART假设决策树是二叉树，内部节点特征的取值为是和否。

### 回归树的生成

1. 遍历所有的变量和所有的切分点。对固定的变量和切分点，可以将样本分成两类。将每一类的回归预测值预测为该类中所有样本点的平均值，这样就可以计算每个样本点的平方误差和，计算被分开的两类的平方误差和。我们要做的是遍历所有变量和切分点使得该值最小。
2. 用选定的变量和切分点划分区域并决定相应的输出值，该类所有样本点的平均值作为该类的预测输出值。
3. 递归前两个步骤

### 分类树的生成

假设有K个类，样本点属于第K类的概率为$P_K$，那么概率分布的基尼指数定义为
$$
Gini(p)=\sum_{k=1}^{K}p_k(1-p_k)
$$
如果样本集合根据特征A是否取某一个值被分割成D1和D2两部分，那么在特征A的条件下，集合D的基尼指数为
$$
Gini(D,A)=|D1|/|D|*Gini(D1)+|D2|/|D|*Gini(D2)
$$
分类生成过程同上。