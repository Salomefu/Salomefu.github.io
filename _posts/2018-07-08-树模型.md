---
xGlayout: post
title: 树模型
subtitle: 
date: 2018-07-08
author: Salome
header-img: img/post-bg-2015.jpg
catalog: true
tags:
    - machine learning
---

[TOC]



# 概况

树是非参数模型，它的参数只要是分裂的特征以及阈值。从所有的决策树中选取最优决策树是NP-Hard问题，所以决策树学习算法通常采用启发式方法，近似求解这一最优化问题。决策树学习算法是一个递归地选择最优特征，并根据该特征对训练数据进行分割的过程。递归进行直至所有样本都被正确分类或者没有更多的特征为止。决策树常用的算法有ID3、C4.5、CART算法。

# ID3和C4.5

ID3和C4.5都是用于分类决策树。ID3通过信息增益选择特征,特征A对训练数据集D的信息增益定义为集合D的经验熵与特征A给定条件下D的经验条件熵之差，等价于类与特征的互信息。
$$
g(D,A)=H(D)-H(D|A)
$$
离散随机变量的熵定义如下,其中n是指该随机变量的取值集合，$p_i$是当随机变量取i值的概率。
$$
H(X)=-\sum_{i=1}^{n}p_ilogp_i
$$
熵越大，不确定性越大，熵的取值范围如下,可见当某个随机变量取值集合越大，那么不确定性越大。
$$
0<=H(X)<=log(n)
$$
条件熵表示在已知随机变量X的情况下Y的不确定性
$$
H(Y|X)=\sum_{i=1}^{n}p_iH(Y|X=X_i)
$$
由于某个特征所能取得值越多，那么条件熵越大。所以用信息增益会偏向于选择取值多的特征，所以在C4.5中用信息增益比替代信息增益。通过除以一个和特征数量有关的系数来去除影响。
$$
g_R(D,A)=g(D,A)/H_A(D)
$$

## 剪枝

决策树的剪枝往往通过极小化决策树整体的损失函数来实现。假设树T的叶节点个数为｜T｜，t是树T的叶节点，该叶节点有$N_t$个样本点，其中k类的样本点有$N_{tk}$个，$H_t(T)$为叶节点上的经验熵，则决策函数的损失函数可以定义为
$$
C(T)=\sum_{t=1}^{|T|}N_tH_t(T)+\alpha|T|
$$
每个节点上的熵（节点样本数作为权重，样本数越多权重越大）与叶子节点的个数的和。第一项代表拟合程度，第二项代表复杂度。剪枝就是通过递归的计算每个节点上有此节点的损失函数值与无此节点的损失函数值的差来确定是否去掉该节点。

# CART

CART假设决策树是二叉树，内部节点特征的取值为是和否，既可以用作回归又可以用作分类。对回归树通过平方误差最小化准则，对分类树采用基尼指数。

## 回归树的生成

1. 遍历所有的变量和所有的切分点。对固定的变量和切分点，可以将样本分成两类。将每一类的回归预测值预测为该类中所有样本点的平均值，这样就可以计算每个样本点的平方误差和，计算被分开的两类的平方误差和。我们要做的是遍历所有变量和切分点使得该值最小。
2. 用选定的变量和切分点划分区域并决定相应的输出值，该类所有样本点的平均值作为该类的预测输出值。
3. 递归前两个步骤

## 分类树的生成

假设有K个类，样本点属于第K类的概率为$P_K$，那么概率分布的基尼指数定义为
$$
Gini(p)=\sum_{k=1}^{K}p_k(1-p_k)
$$
如果样本集合根据特征A是否取某一个值被分割成D1和D2两部分，那么在特征A的条件下，集合D的基尼指数为
$$
Gini(D,A)=|D1|/|D|*Gini(D1)+|D2|/|D|*Gini(D2)
$$
基尼指数越大，样本集合的不确定性也越大。可见用基尼指数作为分类树的特征选择不仅需要选择特征还需要选择特征的具体切分点，这和ID3和C4.5不同。这是因为在CART里面决策树是二叉树，只能根据特征的取值一分为二，而在ID3和C4.5则无此要求。

## 树模型的优缺点

优点：

- 容易解释
- 不要求对特征做预处理，能处理离散值和连续值混合的输入

- 对特征的单调变换不敏感 (只与数据的排序有关) – 能自动进行特征选择
- 可处理缺失数据 
- 可扩展到大数据规模 

缺点：

- 正确率不高:建树过程过于贪心，可作为Boosting的弱学习器(深度不太深) 

- 模型不稳定(方差大):输入数据小的变化会带 来树结构的变化

## Scikit-learn中的tree

CART算法的优化实现

1. 建树:穷举搜索所有特征所有可能取值
2. 没有实现剪枝(采用交叉验证选择最佳的树的参数) 
3. 分类树: DecisionTreeClassifier; 回归树: DecisionTreeRegressor 

```
class sklearn.tree.DecisionTreeClassifier(criterion='gini', splitter='best', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=None, random_state=None, max_leaf_nodes=None, min_impurity_split=1e-07, class_weight=None, presort=False)
```

参数** 

- criterion 

用来权衡划分的质量。缺省‘gini’: 即 Gini impurity。 或者‘entropy’ 

- splitter 

划分方式有三种:best, presort-best, random. 缺省:best 

- max_features 

当进行best划分时，考虑的特征数目(个/比例). 缺省:None 

- max_depth 

树的最大深度。缺省为:None 

- min_samples_split 

对于一个中间节点(internal node)，必须有min个samples才对它进行分割。缺省为:2 

- min_samples_leaf 

每个叶子节点(left node)，至少需要有min_samples_leaf个样本。缺省为:1 

- min_weight_fraction_leaf 

叶子节点的样本权重占总权重的比例。缺省为:0 

- max_leaf_nodes 

以最好优先(best-first)的方式使用该值生成树。如果为None:不限制叶子节点的数目。如果 不为None，则忽略max_depth。缺省为:None 

- class_weight 

每个类别的权重:{class_label: weight}。如果不给定，所有类别的权重均1. “balanced”模式: 自动调整权重。n_samples / (n_classes * np.bincount(y)) 

- random_state 

随机种子 

- presort 

是否对数据进行预先排序，以便在fitting时加快最优划分。对于大数据集，使用False，对于 

小数据集，使用True. 

```
class sklearn.tree.DecisionTreeRegressor(criterion='mse', splitter='best', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=None, random_state=None, max_leaf_nodes=None, min_impurity_split=1e-07, presort=False)
```

与分类树的参数基本相同，*criterion*的缺省值为*‘mse’*，*mean squared error,* 即残差平方和的均值(L2损失) 

# bagging

回归树算法的缺点之一是高方差 

- 一种降低算法方差的方式是平均多个模型的预测:**Bagging** – **B**ootstrap **agg**regat**ing** 

###### bootstrap

对数据集进行放回抽样得到样本

###### bagging

对抽样得到的多个样本训练多个模型再求平均

###### random forest

不仅随机选择样本，还会随机选择特征，降低树的相关性

# boosting

Boost是模型为加法模型，学习算法为前向分步算法的一类算法。

## 前向分步算法

考虑加法模型
$$
f(x)=\sum_{m=1}^{M}\beta_mb(x;\gamma_m)
$$
其中b为基函数。

在给定训练数据及损失函数的条件下，学习加法模型成为损失函数极小化问题
$$
min\sum_{i=1}^{N}L(y_i,\sum_{m=1}^{M}\beta_mb(x_i;\gamma_m))
$$
通常这是一个复杂的优化问题，前向分步算法解决这一问题的思路是：每一步只学习一个基函数及其系数。

1. 初始化$f_0(x)=0$;
2. 对m=1...M

a. 极小化损失函数
$$
argmin_{\beta,\gamma}\sum_{i=1}^{M}L(y_i,f_{m-1(x_i)}+\beta b(x_i;\gamma))
$$
得到参数$\beta_m$,$\gamma_m$

b.更新
$$
f_m(x)=f_{m-1}(x)+\beta_mb(x;\gamma_m)
$$

3. 得到加法模型

$$
f(x)=\sum_{m=1}^{M}\beta_mb(x;\gamma_m)
$$

## Adaboost算法

是前向分步算法的特例，损失函数是指数损失，用于分类问题。

1. 初始化训练数据权值分布D
2. 对m=1...M

a. 使用具有权值分布$D_m$的训练数据根据误差分类率最低学习基本分类器$G_m(x)$

b. 计算基本分类器在数据集上的分类误差率,注意分类误差率是被误分类样本的权值之和。因此要想最小化分类误差率，就得尽量让误分类样本的权值之和小。
$$
e_m=\sum_{i=1}^{N}w_{mi}I(G_m(x_i)!=y_i)
$$
c.计算基本分类器的系数，误误差率越大那么投票表决的系数越小
$$
\alpha_m=1/2log((1-e_m)/e_m)
$$
d.更新训练数据集的权值分布，通过指数函数的构造使得误分类样本的权值得到提高。
$$
w_{m+1,i}=w_{m,i}exp(-\alpha_my_iG_m(x_i))/Z
$$

3. 基本分类器的线性组合

$$
f(x)=\sum_{m=1}^{M}\alpha_mG_m(x)
$$

不改变训练数据，而改变训练数据权值分布，学习得到不同的分类器。

## 提升树

提升方法采用加法模型和前向分步算法，以决策树为基函数的提升方法称为提升树。

对分类的提升树，将adaboost算法中的基函数改为决策树即可。对于回归树的算法叙述如下

当采用平方误差损失时
$$
argmin_{\beta,\gamma}\sum_{i=1}^{M}L(y_i,f_{m-1(x_i)}+\beta T(x_i;\gamma))
$$

$$
L(y,f_{m-1}(x)+T(x;\theta_m))=[y-f_{m-1}(x)-T(x;\theta_m)]^2=[r-T(x;\theta_m)]
$$

所以对于回归提升树来说，只需要简单的拟合当前模型的残差。

### 梯度提升树

当损失函数是指数函数和平方损失函数时，每一步的优化很方便。但是对于一般的损失函数来说，往往每一步优化不是那么简单，因此有了梯度提升算法。其关键是利用损失函数的负梯度作为回归问题提升树算法中的残差的近似值。
$$
r_{mi}=-[\frac{\psi L(y_i,f(x_i))}{\psi f(x_i)}]
$$
对平方损失函数而言，负梯度刚好是残差。对一般的损失函数，负梯度是残差的近似值。

### Scikit-learn中的GBM 

```
sklearn.ensemble.GradientBoostingClassifier(loss='deviance', learning_rate=0.1, n_estimators=100, subsample=1.0, criterion='friedman_mse', min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_depth=3, min_impurity_split=1e-07, init=None, random_state=None, max_features=None, verbose=0, max_leaf_nodes=None, warm_start=False, presort='auto') 
```

| 参数          | 说明                                                         |
| ------------- | ------------------------------------------------------------ |
| loss          | 待优化的目标函数，‘deviance’表示采用logistic损失，输出概率值;‘exponential’ 表示采用指数损失。缺省‘deviance’ |
| learning_rate | 学习率或收缩因子。学习率和迭代次数/弱分类器数目n_estimators相关。 缺省:0.1 |
| n_estimators  | 当数/弱分类器数目. 缺省:100                                  |
| subsample     | 学习单个弱学习器的样本比例。缺省为:1.0                       |

### XGboost

xgboost也是一种提升树。

目标函数定义为
$$
Obj=\sum_{i=1}^{n}l(y_i,\hat y_i^{t-1}+f_t(X_i))+\sum_{k=1}^{K}\phi(f_k)+const
$$

$$
\phi(f_k)=\gamma T+\frac{1}{2}\lambda||w||^2
$$

第一部分衡量损失，另外一部分是正则项,正则项由两部分构成，T代表叶子结点的个数，w代表叶子节点的分数。

接下来我们要找到一个f(x)使得目标函数最小化。XGboost利用泰勒二阶展开近似原函数
$$
L_t=\sum_{i=1}^{n}[l(y_i,\hat y^{t-1})+g_if_t(X_i)+\frac{1}{2}h_if_t^2(X_i)]+\phi(f_t)
$$

$$
g_i=\psi_{\hat y^{t-1}}l(y_i,\hat y^{t-1})
$$

$h_i$是对应的二阶导数，由于前t-1棵树的预测分数与y的残差对当前目标函数优化没有影响，可以直接去掉。于是目标函数为
$$
L_t=\sum_{i=1}^{n}[g_if_t(X_i)+\frac{1}{2}h_if_t^2(X_i)]+\phi(f_t)
$$
考虑损失函数是平方损失，那么
$$
Obj=\sum_{i=1}^{n}l(y_i-(\hat y_i^{t-1}+f_t(X_i)))^2+\phi(f_k)+const
$$

$$
Obj=\sum_{i=1}^{n}[2(\hat y_i^{t-1}-y_i)f_t(x_i)+f_t(x_i)^2]+\phi(f_k)+const
$$

此时的$g_i=2(\hat y^{t-1}-h_i)$,$h_i$=2

上式将每个叶子节点的损失函数值加起来，我们知道，每个样本最终都会落入一个叶子结点中，所以我们可以将同一个叶子结点的样本重组起来
$$
Obj^{t}=\sum_{i=1}^{n}[g_iw_{q(x_i)}+\frac{1}{2}h_iw_{q(x_i)}^2]+\gamma T+\lambda\frac{1}{2}\sum_{j=1}^Tw_j^2
$$

$$
Obj^{t}=\sum_{j=1}^{T}[(\sum_{i\in I_j}g_i)w_j+\frac{1}{2}(\sum_{i\in I_j}h_i+\lambda)w_j^2]+\lambda T
$$

通过上式的改写，我们可以将目标函数改写成关于叶子结点分数w的一个一元二次函数，求解最优的w和目标函数值就变得很简单了，直接使用顶点公式即可。因此，最优的w和目标函数公式为



![img](https://tva1.sinaimg.cn/large/006tNbRwgy1gaeu82es1ej30fe01ft8k.jpg)

#### 建树流程

假设可能的话，我们可以枚举所有的树结构，计算结构分数Obj。然后选择分数最小的树架构，然后运用最优的分数w即可。但由于这是一个NPhard问题，所以只能用贪心算法建树。从深度为0的树开始，对于树的每个叶子节点，尝试增加一个分裂点使得增加分裂点后目标函数的变化为
$$
gain=\frac{G_L^2}{H_L+\lambda}+\frac{G_R^2}{H_R+\lambda}-\frac{G_L^2+G_R^2}{H_L+H_R+\lambda}-\gamma
$$


#### 如何划分特征以及切分点

##### 精确搜索法

对每一个结点，穷举所有特征、所有可能的分裂点，找到使得增益最大的节点

1. 对每个特征，通过特征值将实例进行排序
2. 运用线性扫描来寻找该特征的最优分裂点
3. 对所有特征，采用最佳分裂点 

##### 近似搜索法

当数据太多不能装载到内存时，不能进行精确搜索分裂，只能近似 

1. 根据特征分布的百分位数，提出特征的一些候选分裂点 

2. 将连续特征值映射到桶里(候选点对应的分裂)，然后根据桶里样本的统计量，从这些候选中选择最佳分裂点 

#### 一些优化的地方

##### Shrinkage and Column Subsampling

XGBoost还提出了两种防止过拟合的方法：Shrinkage and Column Subsampling。Shrinkage方法就是在每次迭代中对树的每个叶子结点的分数乘上一个缩减权重η，这可以使得每一棵树的影响力不会太大，留下更大的空间给后面生成的树去优化模型。

Column Subsampling类似于随机森林中的选取部分特征进行建树。其可分为两种，一种是按层随机采样，在对同一层内每个结点分裂之前，先随机选择一部分特征，然后只需要遍历这部分的特征，来确定最优的分割点。另一种是随机选择特征，则建树前随机选择一部分特征然后分裂就只遍历这些特征。一般情况下前者效果更好。

##### 近似算法

对于连续型特征值，当样本数量非常大，该特征取值过多时，遍历所有取值会花费很多时间，且容易过拟合。因此XGBoost思想是对特征进行分桶，即找到l个划分点，将位于相邻分位点之间的样本分在一个桶中。在遍历该特征的时候，只需要遍历各个分位点，从而计算最优划分。从算法伪代码中该流程还可以分为两种，全局的近似是在新生成一棵树之前就对各个特征计算分位点并划分样本，之后在每次分裂过程中都采用近似划分，而局部近似就是在具体的某一次分裂节点的过程中采用近似算法。

##### 针对稀疏数据的算法（缺失值处理）

当样本的第i个特征值缺失时，无法利用该特征进行划分时，XGBoost的想法是将该样本分别划分到左结点和右结点，然后计算其增益，哪个大就划分到哪边。

#### XGBoost的优点

1. 使用许多策略去防止过拟合，如：正则化项、Shrinkage and Column Subsampling等。

2. 目标函数优化利用了损失函数关于待求函数的二阶导数

3. 支持并行化，这是XGBoost的闪光点，虽然树与树之间是串行关系，但是同层级节点可并行。具体的对于某个节点，节点内选择最佳分裂点，候选分裂点计算增益用多线程并行。训练速度快。

4. 添加了对稀疏数据的处理。

5. 交叉验证，early stop，当预测结果已经很好的时候可以提前停止建树，加快训练速度。

6. 支持设置样本权重，该权重体现在一阶导数g和二阶导数h，通过调整权重可以去更加关注一些样本。

#### 和GBDT的区别

1. 传统GBDT只用到一阶导信息，xgb既利用一阶导又用到二阶导，支持自定义损失函数，只要函数一阶和二阶可导
2. xgb在目标函数里加入了正则项，用于控制模型的复杂度。正则项里包含了树的叶子节点个数、每个叶子节点上输出的score的L2模的平方和
3. 并行化处理，xgboost的并行不是tree粒度的并行，xgboost也是一次迭代完才能进行下一次迭代的（第t次迭代的代价函数里包含了前面t-1次迭代的预测值）。xgboost的并行是在特征粒度上的。我们知道，决策树的学习最耗时的一个步骤就是对特征的值进行排序（因为要确定最佳分割点），xgboost在训练之前，预先对数据进行了排序，然后保存为block结构，后面的迭代中重复地使用这个结构，大大减小计算量。这个block结构也使得并行成为了可能，在进行节点的分裂时，需要计算每个特征的增益，最终选增益最大的那个特征去做分裂，那么各个特征的增益计算就可以开多线程进行。
4. 内置交叉验证，xgboost在每一轮boosting迭代过程中使用交叉验证，这样我们可以方便的获取最优迭代次数。

```
xgboost.XGBClassifier(max_depth=3, learning_rate=0.1, n_estimators=100, silent=True, objective=‘binary:logistic’, nthread=-1, gamma=0, min_child_weight=1, max_delta_step=0, subsample=1, colsample_bytree=1, colsample_bylevel=1, reg_alpha=0, reg_lambda=1, scale_pos_weight=1, base_score=0.5, random_state=0,seed=None,missing=None，**kwargs) 
```

#### 特征工程

XGBoost 模型内部将所有的问题都建模成一个回归预测问题，输入特征只能是数值型。 

如果给定的数据是不同的类型，必须先将数据变成数值型。 

##### 类别型特征

- LabelEncoder: 对不连续的数字或者文本进行编号

  - 可用在对字符串型的标签编码(测试结果需进行反变换) 

  -  编号默认有序数关系 

  - 存储量小 

- 如不希望有序数关系: OneHotEncoder:将类别型整数输入从1维K 维的稀疏编码 

  - 对XGBoost，OneHotEncoder不是必须，因为XGBoost对特征进行排序从 而进行分裂建树;如果用OneHotEncoder得到稀疏编码，XGBoost建树过程中 对稀疏特征处理速度块 

  - 输入必须是数值型数据(对字符串输入，先调用LabelEncoder变成数字，再用 OneHotEncoder 
  - 存储要求高 

- 低基数(low-cardinality )类别型特征: OneHotEncoder – 1维K维，K为该特征不同的取值数目,所以通常在K<10的情况下采用 

- 高基数(high-cardinality)类别型特征:通常有成百上千个不同的取值，可先降维.如邮政编码、街道名称... 

  - 聚类(Clustering)

  - 主成分分析(principlecomponent analysis, PCA):但对大矩阵操作费资源 

  - 均值编码:在贝叶斯的架构下, 利用标签变量,有监督地确定最适合特定特征的编码方式 

    均值编码:将特征每个可能的取值k(共有K种取值)，编码为它对应的标签取c值的概率$p(y=c|x=k)$

##### 日期型特征

- 日期特征:年月日
- 时间特征:小时分秒 
-  时间段:早中晚
- 星期，工作日/周末 

- ```python
  train['created'] = pd.to_datetime(train['created']) 
  train['date'] = train['created'].dt.date 
  train["year"] = train["created"].dt.year 
  train['month'] = train['created'].dt.month 
  train['day'] = train['created'].dt.day 
  train['hour'] = train['created'].dt.hour 
  train['weekday'] = train['created'].dt.weekday 
  train['week'] = train['created'].dt.week 
  train['quarter'] = train['created'].dt.quarter 
  train['weekend'] = ((train['weekday'] == 5) & (train['weekday'] == 6))
  train['wd'] = ((train['weekday'] != 5) & (train['weekday'] != 6)) 
  ```

  

#### 调节参数

##### 参数类别 

- 通用参数:这部分参数通常我们不需要调整，默认值就好 

- 学习目标参数:与任务有关，定下来后通常也不需要调整 

- booster参数:弱学习器相关参数，需要仔细调整，会影响 模型性能 

###### 通用参数 

- booster:弱学习器类型
   – 可选gbtree(树模型)或gbliner(线性模型)
   – 默认为gbtree(树模型为非线性模型，能处理更复杂的任务) 

- silent:是否开启静默模式
   – 1:静默模式开启，不输出任何信息
   – 默认值为0:输出一些中间信息，以助于我们了解模型的状态 

- nthread:线程数
   – 默认值为-1，表示使用系统所有CPU核 

###### 学习目标参数 

- objective: 损失函数
   – 支持分类/回归/排序 

- eval_metric:评价函数 

- seed:随机数的种子 

  – 默认为0 

  – 设置seed可复现随机数据的结果，也可以用于调整参数 

###### booster参数

弱学习器的参数，尽管有两种booster可供选择，这里只介绍gbtree 

- learning_rate : 收缩步长 vs. n_estimators:树的数目
   – 较小的学习率通常意味着更多弱分学习器
   – 通常建议学习率较小( 𝜂 < 0.1)，弱学习器数目n_estimators大 
  $$
  f_m(x)=f_{m-1}(x)+𝜂\beta_m\phi_m(x)
  $$
   – 可以设置较小的学习率，然后用交叉验证确定n_estimators 

- 行(subsample) 列(colsample_bytree、 colsample_bylevel)下采样比例 

– 默认值均为1，即不进行下采样，使用所有数据
 – 随机下采样通常比用全部数据的确定性过程效果更好，速度更快

 – 建议值:0.3 - 0.8 

- 树的最大深度: max_depth
   – max_depth越大，模型越复杂，会学到更具体更局部的样本 

  – 需要使用交叉验证进行调优，默认值为6，建议3-10 

- min_child_weight :孩子节点中最小的样本权重和 如果一个叶子节点的样本权重和小于min_child_weight则分裂过程结束

##### 参数调优的方法

1. 选择较高的学习率(learning rate)，并选择对应于此学习率 的理想的树数量 

- 学习率以工具包默认值为0.1。 

- XGBoost直接引用函数“cv”可以在每一次迭代中使用交叉验证，并返回理想的树数量(因为交叉验证很慢，所  以可以import两种XGBoost:直接引用 xgboost(用“cv”函数调整树的数目)和XGBClassifier —xgboost的sklearn包 (用GridSearchCV调整其他参数 )。 

2. ```
   采用缺省参数,此时learning_rate =0.1(较大)，观察 n_estimators的合适范围 
   – 参数设为1000，earlystop = 50
   – cv函数在n_estimators =699时停止 
   ```

2. 对于给定的学习率和树数量，进行树参数调优(max_depth, min_child_weight, gamma, subsample, colsample_bytree, colsample_bylevel) 

```
max_depth 和 min_child_weight 参数调整
– 这两个参数对结果影响很大
– 我们先大范围地粗调参数(步长为2)，再小范围微调
– max_depth=range(3,10,2)
– min_child_weight = range(1,6,2)

max_depth 和 min_child_weight 参数微调 
– 在max_depth=7和min_child_weight=5周围微调
– max_depth = [6,7,8]
– min_child_weight = [4,5,6]

调整max_depth=6 和 min_child_weight=4后，再次调整 n_estimators

gamma参数调整，通常缺省值(0)表现还不错

行列采样参数: subsample和colsample_bytree
– 这两个参数可分别对样本和特征进行采样，从而增加模型的鲁棒性
– 现在[0.3-1.0]之间调整，粗调时步长0.1
– 微调时步长为0.05(略)
```

3. xgboost的正则化参数(lambda, alpha)的调优 
4. 降低学习率，确定理想参数 