---
Glayout: post
title: 树模型
subtitle: 
date: 2018-07-08
author: Salome
header-img: img/post-bg-2015.jpg
catalog: true
tags:
    - machine learning
---

[TOC]



# 概况

从所有的决策树中选取最优决策树是NP-Hard问题，所以决策树学习算法通常采用启发式方法，近似求解这一最优化问题。决策树学习算法是一个递归地选择最优特征，并根据该特征对训练数据进行分割的过程。递归进行直至所有样本都被正确分类或者没有更多的特征为止。决策树常用的算法有ID3、C4.5、CART算法。

# ID3和C4.5

ID3和C4.5都是用于分类决策树。ID3通过信息增益选择特征,特征A对训练数据集D的信息增益定义为集合D的经验熵与特征A给定条件下D的经验条件熵之差，等价于类与特征的互信息。
$$
g(D,A)=H(D)-H(D|A)
$$
离散随机变量的熵定义如下,其中n是指该随机变量的取值集合，$p_i$是当随机变量取i值的概率。
$$
H(X)=-\sum_{i=1}^{n}p_ilogp_i
$$
熵越大，不确定性越大，熵的取值范围如下,可见当某个随机变量取值集合越大，那么不确定性越大。
$$
0<=H(X)<=log(n)
$$
条件熵表示在已知随机变量X的情况下Y的不确定性
$$
H(Y|X)=\sum_{i=1}^{n}p_iH(Y|X=X_i)
$$
由于某个特征所能取得值越多，那么条件熵越大。所以用信息增益会偏向于选择取值多的特征，所以在C4.5中用信息增益比替代信息增益。通过除以一个和特征数量有关的系数来去除影响。
$$
g_R(D,A)=g(D,A)/H_A(D)
$$

## 剪枝

决策树的剪枝往往通过极小化决策树整体的损失函数来实现。假设树T的叶节点个数为｜T｜，t是树T的叶节点，该叶节点有$N_t$个样本点，其中k类的样本点有$N_{tk}$个，$H_t(T)$为叶节点上的经验熵，则决策函数的损失函数可以定义为
$$
C(T)=\sum_{t=1}^{|T|}N_tH_t(T)+\alpha|T|
$$
每个节点上的熵（节点样本数作为权重，样本数越多权重越大）与叶子节点的个数的和。第一项代表拟合程度，第二项代表复杂度。剪枝就是通过递归的计算每个节点上有此节点的损失函数值与无此节点的损失函数值的差来确定是否去掉该节点。

# CART

CART假设决策树是二叉树，内部节点特征的取值为是和否，既可以用作回归又可以用作分类。对回归树通过平方误差最小化准则，对分类树采用基尼指数。

## 回归树的生成

1. 遍历所有的变量和所有的切分点。对固定的变量和切分点，可以将样本分成两类。将每一类的回归预测值预测为该类中所有样本点的平均值，这样就可以计算每个样本点的平方误差和，计算被分开的两类的平方误差和。我们要做的是遍历所有变量和切分点使得该值最小。
2. 用选定的变量和切分点划分区域并决定相应的输出值，该类所有样本点的平均值作为该类的预测输出值。
3. 递归前两个步骤

## 分类树的生成

假设有K个类，样本点属于第K类的概率为$P_K$，那么概率分布的基尼指数定义为
$$
Gini(p)=\sum_{k=1}^{K}p_k(1-p_k)
$$
如果样本集合根据特征A是否取某一个值被分割成D1和D2两部分，那么在特征A的条件下，集合D的基尼指数为
$$
Gini(D,A)=|D1|/|D|*Gini(D1)+|D2|/|D|*Gini(D2)
$$
基尼指数越大，样本集合的不确定性也越大。可见用基尼指数作为分类树的特征选择不仅需要选择特征还需要选择特征的具体切分点，这和ID3和C4.5不同。这是因为在CART里面决策树是二叉树，只能根据特征的取值一分为二，而在ID3和C4.5则无此要求。



# boosting

Boost是模型为加法模型，学习算法为前向分步算法的一类算法。

## 前向分步算法

考虑加法模型
$$
f(x)=\sum_{m=1}^{M}\beta_mb(x;\gamma_m)
$$
其中b为基函数。

在给定训练数据及损失函数的条件下，学习加法模型成为损失函数极小化问题
$$
min\sum_{i=1}^{N}L(y_i,\sum_{m=1}^{M}\beta_mb(x_i;\gamma_m))
$$
通常这是一个复杂的优化问题，前向分步算法解决这一问题的思路是：每一步只学习一个基函数及其系数。

1. 初始化$f_0(x)=0$;
2. 对m=1...M

a. 极小化损失函数
$$
argmin_{\beta,\gamma}\sum_{i=1}^{M}L(y_i,f_{m-1(x_i)}+\beta b(x_i;\gamma))
$$
得到参数$\beta_m$,$\gamma_m$

b.更新
$$
f_m(x)=f_{m-1}(x)+\beta_mb(x;\gamma_m)
$$

3. 得到加法模型

$$
f(x)=\sum_{m=1}^{M}\beta_mb(x;\gamma_m)
$$

## Adaboost算法

是前向分步算法的特例，损失函数是指数损失，用于分类问题。

1. 初始化训练数据权值分布D
2. 对m=1...M

a. 使用具有权值分布$D_m$的训练数据根据误差分类率最低学习基本分类器$G_m(x)$

b. 计算基本分类器在数据集上的分类误差率,注意分类误差率是被误分类样本的权值之和。因此要想最小化分类误差率，就得尽量让误分类样本的权值之和小。
$$
e_m=\sum_{i=1}^{N}w_{mi}I(G_m(x_i)!=y_i)
$$
c.计算基本分类器的系数，误误差率越大那么投票表决的系数越小
$$
\alpha_m=1/2log((1-e_m)/e_m)
$$
d.更新训练数据集的权值分布，通过指数函数的构造使得误分类样本的权值得到提高。
$$
w_{m+1,i}=w_{m,i}exp(-\alpha_my_iG_m(x_i))/Z
$$

3. 基本分类器的线性组合

$$
f(x)=\sum_{m=1}^{M}\alpha_mG_m(x)
$$

不改变训练数据，而改变训练数据权值分布，学习得到不同的分类器。

## 提升树

提升方法采用加法模型和前向分步算法，以决策树为基函数的提升方法称为提升树。

对分类的提升树，将adaboost算法中的基函数改为决策树即可。对于回归树的算法叙述如下

当采用平方误差损失时
$$
argmin_{\beta,\gamma}\sum_{i=1}^{M}L(y_i,f_{m-1(x_i)}+\beta T(x_i;\gamma))
$$

$$
L(y,f_{m-1}(x)+T(x;\theta_m))=[y-f_{m-1}(x)-T(x;\theta_m)]^2=[r-T(x;\theta_m)]
$$

所以对于回归提升树来说，只需要简单的拟合当前模型的残差。

### 梯度提升树

当损失函数是指数函数和平方损失函数时，每一步的优化很方便。但是对于一般的损失函数来说，往往每一步优化不是那么简单，因此有了梯度提升算法。其关键是利用损失函数的负梯度作为回归问题提升树算法中的残差的近似值。
$$
r_{mi}=-[\frac{\psi L(y_i,f(x_i))}{\psi f(x_i)}]
$$
对平方损失函数而言，负梯度刚好是残差。对一般的损失函数，负梯度是残差的近似值。

### XGboost

xgboost也是一种提升树。

目标函数定义为
$$
Obj=\sum_{i=1}^{n}l(y_i,\hat y_i^{t-1}+f_t(X_i))+\sum_{k=1}^{K}\phi(f_k)+const
$$

$$
\phi(f_k)=\gamma T+\frac{1}{2}\lambda||w||^2
$$

第一部分衡量损失，另外一部分是正则项,正则项由两部分构成，T代表叶子结点的个数，w代表叶子节点的分数。

接下来我们要找到一个f(x)使得目标函数最小化。XGboost利用泰勒二阶展开近似原函数
$$
L_t=\sum_{i=1}^{n}[l(y_i,\hat y^{t-1})+g_if_t(X_i)+\frac{1}{2}h_if_t^2(X_i)]+\phi(f_t)
$$

$$
g_i=\psi_{\hat y^{t-1}}l(y_i,\hat y^{t-1})
$$

$h_i$是对应的二阶导数，由于前t-1棵树的预测分数与y的残差对当前目标函数优化没有影响，可以直接去掉。于是目标函数为
$$
L_t=\sum_{i=1}^{n}[g_if_t(X_i)+\frac{1}{2}h_if_t^2(X_i)]+\phi(f_t)
$$
考虑损失函数是平方损失，那么
$$
Obj=\sum_{i=1}^{n}l(y_i-(\hat y_i^{t-1}+f_t(X_i)))^2+\phi(f_k)+const
$$

$$
Obj=\sum_{i=1}^{n}[2(\hat y_i^{t-1}-y_i)f_t(x_i)+f_t(x_i)^2]+\phi(f_k)+const
$$

此时的$g_i=2(\hat y^{t-1}-h_i)$,$h_i$=2

上式将每个叶子节点的损失函数值加起来，我们知道，每个样本最终都会落入一个叶子结点中，所以我们可以将同一个叶子结点的样本重组起来
$$
Obj^{t}=\sum_{i=1}^{n}[g_iw_{q(x_i)}+\frac{1}{2}h_iw_{q(x_i)}^2]+\gamma T+\lambda\frac{1}{2}\sum_{j=1}^Tw_j^2
$$

$$
Obj^{t}=\sum_{j=1}^{T}[(\sum_{i\in I_j}g_i)w_j+\frac{1}{2}(\sum_{i\in I_j}h_i+\lambda)w_j^2]+\lambda T
$$

通过上式的改写，我们可以将目标函数改写成关于叶子结点分数w的一个一元二次函数，求解最优的w和目标函数值就变得很简单了，直接使用顶点公式即可。因此，最优的w和目标函数公式为



![img](https://tva1.sinaimg.cn/large/006tNbRwgy1gaeu82es1ej30fe01ft8k.jpg)

#### 如何划分特征以及切分点

对每个特征以及划分点根据划分前后目标函数的值变化情况确定，找到使得目标函数最小的特征及其切分点。找到切分特征以及切分点之后，每个节点上的值也被确定了($w_j$)

#### 一些优化的地方

##### Shrinkage and Column Subsampling

XGBoost还提出了两种防止过拟合的方法：Shrinkage and Column Subsampling。Shrinkage方法就是在每次迭代中对树的每个叶子结点的分数乘上一个缩减权重η，这可以使得每一棵树的影响力不会太大，留下更大的空间给后面生成的树去优化模型。Column Subsampling类似于随机森林中的选取部分特征进行建树。其可分为两种，一种是按层随机采样，在对同一层内每个结点分裂之前，先随机选择一部分特征，然后只需要遍历这部分的特征，来确定最优的分割点。另一种是随机选择特征，则建树前随机选择一部分特征然后分裂就只遍历这些特征。一般情况下前者效果更好。

##### 近似算法

对于连续型特征值，当样本数量非常大，该特征取值过多时，遍历所有取值会花费很多时间，且容易过拟合。因此XGBoost思想是对特征进行分桶，即找到l个划分点，将位于相邻分位点之间的样本分在一个桶中。在遍历该特征的时候，只需要遍历各个分位点，从而计算最优划分。从算法伪代码中该流程还可以分为两种，全局的近似是在新生成一棵树之前就对各个特征计算分位点并划分样本，之后在每次分裂过程中都采用近似划分，而局部近似就是在具体的某一次分裂节点的过程中采用近似算法。

##### 针对稀疏数据的算法（缺失值处理）

当样本的第i个特征值缺失时，无法利用该特征进行划分时，XGBoost的想法是将该样本分别划分到左结点和右结点，然后计算其增益，哪个大就划分到哪边。

#### XGBoost的优点

1. 使用许多策略去防止过拟合，如：正则化项、Shrinkage and Column Subsampling等。

2. 目标函数优化利用了损失函数关于待求函数的二阶导数

3. 支持并行化，这是XGBoost的闪光点，虽然树与树之间是串行关系，但是同层级节点可并行。具体的对于某个节点，节点内选择最佳分裂点，候选分裂点计算增益用多线程并行。训练速度快。

4. 添加了对稀疏数据的处理。

5. 交叉验证，early stop，当预测结果已经很好的时候可以提前停止建树，加快训练速度。

6. 支持设置样本权重，该权重体现在一阶导数g和二阶导数h，通过调整权重可以去更加关注一些样本。

#### 和GBDT的区别

1. 传统GBDT只用到一阶导信息，xgb既利用一阶导又用到二阶导，支持自定义损失函数，只要函数一阶和二阶可导
2. xgb在目标函数里加入了正则项，用于控制模型的复杂度。正则项里包含了树的叶子节点个数、每个叶子节点上输出的score的L2模的平方和
3. 并行化处理，xgboost的并行不是tree粒度的并行，xgboost也是一次迭代完才能进行下一次迭代的（第t次迭代的代价函数里包含了前面t-1次迭代的预测值）。xgboost的并行是在特征粒度上的。我们知道，决策树的学习最耗时的一个步骤就是对特征的值进行排序（因为要确定最佳分割点），xgboost在训练之前，预先对数据进行了排序，然后保存为block结构，后面的迭代中重复地使用这个结构，大大减小计算量。这个block结构也使得并行成为了可能，在进行节点的分裂时，需要计算每个特征的增益，最终选增益最大的那个特征去做分裂，那么各个特征的增益计算就可以开多线程进行。