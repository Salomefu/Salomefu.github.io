---
layout: post
title: 大数据之Hadoop集群搭建
subtitle: 
date: 2019-01-02
author: Salome
header-img: img/post-bg-2015.jpg
catalog: true
tags:

   - big data
---



之前陆陆续续的学习和使用过Hadoop集群相关组件，但都不成体系。这次从搭建大数据集群开始，详细的记录下学习过程。

# 1. Hadoop生态圈

Hadoop是一个分布式系统基础架构，主要解决海量数据的存储和海量数据的分析计算问题。Hadoop2.x有四个组件，分别是MapReduce(负责计算)、Yarn(资源调度)、HDFS(数据存储)、Common(辅助工具)。  

#### HDFS架构

![HDFS](https://hadoop.apache.org/docs/r1.2.1/images/hdfsarchitecture.gif)

#### YARN架构

![YARN](https://data-flair.training/blogs/wp-content/uploads/sites/2/2017/05/Apache-YARN-architecture-min.jpg)

#### MapReduce计算流程

![mapreduce](https://bigdatapath.files.wordpress.com/2018/04/12.png?w=660)

#### Hadoop整体生态系统

![hadoop](https://www.edureka.co/blog/wp-content/uploads/2016/10/HADOOP-ECOSYSTEM-Edureka.png)

图中涉及的技术名词解释如下：

1）Sqoop：Sqoop是一款开源的工具，主要用于在Hadoop、Hive与传统的数据库(MySql)间进行数据的传递，可以将一个关系型数据库（例如 ：MySQL，Oracle 等）中的数据导进到Hadoop的HDFS中，也可以将HDFS的数据导进到关系型数据库中。  

2）Flume：Flume是Cloudera提供的一个高可用的，高可靠的，分布式的海量日志采集、聚合和传输的系统，Flume支持在日志系统中定制各类数据发送方，用于收集数据；同时，Flume提供对数据进行简单处理，并写到各种数据接受方（可定制）的能力。

3）Kafka：Kafka是一种高吞吐量的分布式发布订阅消息系统，有如下特性：

（1）通过O(1)的磁盘数据结构提供消息的持久化，这种结构对于即使数以TB的消息存储也能够保持长时间的稳定性能。

（2）高吞吐量：即使是非常普通的硬件Kafka也可以支持每秒数百万的消息。

（3）支持通过Kafka服务器和消费机集群来分区消息。

（4）支持Hadoop并行数据加载。

4）Storm：Storm用于“连续计算”，对数据流做连续查询，在计算时就将结果以流的形式输出给用户。

5）Spark：Spark是当前最流行的开源大数据内存计算框架。可以基于Hadoop上存储的大数据进行计算。

6）Oozie：Oozie是一个管理Hdoop作业（job）的工作流程调度管理系统。

7）Hbase：HBase是一个分布式的、面向列的开源数据库。HBase不同于一般的关系数据库，它是一个适合于非结构化数据存储的数据库。

8）Hive：Hive是基于Hadoop的一个数据仓库工具，可以将结构化的数据文件映射为一张数据库表，并提供简单的SQL查询功能，可以将SQL语句转换为MapReduce任务进行运行。 其优点是学习成本低，可以通过类SQL语句快速实现简单的MapReduce统计，不必开发专门的MapReduce应用，十分适合数据仓库的统计分析。

9）Mahout：Apache Mahout是个可扩展的机器学习和数据挖掘库。

10）ZooKeeper：Zookeeper是Google的Chubby一个开源的实现。它是一个针对大型分布式系统的可靠协调系统，提供的功能包括：配置维护、名字服务、 分布式同步、组服务等。ZooKeeper的目标就是封装好复杂易出错的关键服务，将简单易用的接口和性能高效、功能稳定的系统提供给用户。

# 2. Hadoop虚拟机集群搭建

## 2.1 虚拟机环境准备

系统配置如下：  

1. Mac pro 16G内存 256G存储
2. 虚拟机为Vmware Fusion
3. Linux系统为Centos7

##### 准备Linux环境

1. 虚拟机安装比较简单，注意网络选择NAT模式即可。有个关键的步骤为配置主机的静态IP，否则无法上网。详细步骤参见[https://www.cnblogs.com/itbsl/p/10998696.html](https://www.cnblogs.com/itbsl/p/10998696.html)。
2. 接下来修改主机名并且关闭防火墙。使用虚拟机的克隆功能克隆另外两台主机并注意修改主机名,我的三台主机名分别为Hadoop100、Hadoop101、Hadoop102。注意配置启动http服务。
3. 配置三台主机ssh免密登陆
4. 使用Secure CRT软件远程连接三台主机
5. 接下来就是正式安装Hadoop环境啦～

## 2.2 Hadoop集群软件安装

```powershell
#（1）在/opt目录下创建module、software文件夹
[root@hadoop100 opt]$ sudo mkdir module
[root@hadoop100 opt]$ sudo mkdir software
#（2）查询是否安装Java软件
[root@hadoop100 opt]$ rpm -qa | grep java
#    如果安装的版本低于1.7，卸载该JDK：
[root@hadoop100 opt]$ sudo rpm -e 软件包
#    查看JDK安装路径：
[root@hadoop100 ~]$ which java
# (3)用SecureCRT工具将JDK导入到opt目录下面的software文件夹下面
[root@hadoop100 opt]$ cd software/
[root@hadoop100 software]$ ls
hadoop-2.7.2.tar.gz  jdk-8u144-linux-x64.tar.gz
# (4)解压JDK到/opt/module目录下
[root@hadoop100 software]$ tar -zxvf jdk-8u144-linux-x64.tar.gz -C /opt/module/
# (5)配置JDK环境变量
#  先获取JDK路径
[root@hadoop100 jdk1.8.0_144]$ pwd
/opt/module/jdk1.8.0_144
#  打开/etc/profile文件
[root@hadoop100 software]$ sudo vi /etc/profile
#. 在profile文件末尾添加JDK路径
export JAVA_HOME=/opt/module/jdk1.8.0_144
export PATH=$PATH:$JAVA_HOME/bin
# （6）让修改后的文件生效
[root@hadoop100 jdk1.8.0_144]$ source /etc/profile
# 测试JDK是否安装成功
[root@hadoop100 jdk1.8.0_144]# java -version
java version "1.8.0_144"
#  (7)进入到Hadoop安装包路径下
[root@hadoop100 ~]$ cd /opt/software/
#  (8)解压安装文件到/opt/module下面
[root@hadoop100 software]$ tar -zxvf hadoop-2.7.2.tar.gz -C /opt/module/
#  (9)查看是否解压成功
[root@hadoop100 software]$ ls /opt/module/
hadoop-2.7.2
#  将Hadoop添加到环境变量 获取Hadoop安装路径
[root@hadoop100 hadoop-2.7.2]$ pwd
/opt/module/hadoop-2.7.2
#. 打开/etc/profile文件
[root@hadoop100 hadoop-2.7.2]$ sudo vi /etc/profile
# 在profile文件末尾添加JDK路径：（shitf+g）
export HADOOP_HOME=/opt/module/hadoop-2.7.2
export PATH=$PATH:$HADOOP_HOME/bin
export PATH=$PATH:$HADOOP_HOME/sbin
# 让修改后的文件生效
[root@ hadoop100 hadoop-2.7.2]$ source /etc/profile
# 测试是否安装成功
[root@hadoop100 hadoop-2.7.2]$ hadoop version
Hadoop 2.7.2
```

   hadoop的重要目录如下  

（1）bin目录：存放对Hadoop相关服务（HDFS,YARN）进行操作的脚本

（2）etc目录：Hadoop的配置文件目录，存放Hadoop的配置文件

（3）lib目录：存放Hadoop的本地库（对数据进行压缩解压缩功能）

（4）sbin目录：存放启动或停止Hadoop相关服务的脚本

（5）share目录：存放Hadoop的依赖jar包、文档、和官方案例

## 2.2 集群管理之分发脚本

实现服务器与服务器通信有一下几种方法

1. scp（secure copy）安全拷贝

（1）scp定义：

scp可以实现服务器与服务器之间的数据拷贝。（from server1 to server2）  

（2）基本语法

scp  -r    要拷贝的文件路径/名称  目的用户@主机:目的路径/名称   

（3）案例实操

在hadoop100上，将hadoop100中/opt/module目录下的软件拷贝到hadoop101上。

```powershell
[root@hadoop100 /]$ scp -r /opt/module root@hadoop101:/opt/module
```

2. rsync 远程同步工具

rsync主要用于备份和镜像。具有速度快、避免复制相同内容和支持符号链接的优点。

rsync和scp区别：用rsync做文件的复制要比scp的速度快，rsync只对差异文件做更新。scp是把所有文件都复制过去。

（1）基本语法

rsync   -av    要拷贝的文件路径/名称  目的用户@主机:目的路径/名称

| 选项 | 功能         |
| ---- | ------------ |
| -a   | 归档拷贝     |
| -v   | 显示复制过程 |

（2）案例实操

把hadoop100机器上的/opt/software目录同步到hadoop102服务器的root用户下的/opt/目录

```powershell
[root@hadoop100 opt]$ rsync -av /opt/software/ hadoop102:/opt/software
```

3. xsync集群分发脚本

（1）需求：循环复制文件到所有节点的相同目录下

（2）需求分析：

（a）rsync命令原始拷贝：rsync -av   /opt/module        root@hadoop103:/opt/

（b）期望脚本运行方式：xsync   要同步的文件名称

（c）说明：在/home/root/bin这个目录下存放的脚本，root用户可以在系统任何地方直接执行。

（3）脚本实现

（a）在/home/root目录下创建bin目录，并在bin目录下创建脚本文件xsync

```powershell
[root@hadoop100 ~]$ mkdir bin
[root@hadoop100 ~]$ cd bin/
[root@hadoop100 bin]$ touch xsync
[root@hadoop100 bin]$ vi xsync
```

(b)   在该文件中编写如下shell代码

```shell
#!/bin/bash
#1 获取输入参数个数，如果没有参数，直接退出
pcount=$#
if ((pcount==0)); then
echo no args;
exit;
fi
#2 获取文件名称
p1=$1
fname=`basename $p1`
echo fname=$fname
#3 获取上级目录到绝对路径
pdir=`cd -P $(dirname $p1); pwd`
echo pdir=$pdir
#4 获取当前用户名称
user=`whoami`
#5 循环100、101、102主机分发文件
for((host=100; host<103; host++)); do
echo ------------------- hadoop$host --------------
rsync -av $pdir/$fname $user@hadoop$host:$pdir
done
```

（c）修改脚本 xsync 具有执行权限

```powershell
[root@hadoop100 bin]$ chmod 777 xsync
```

（d）调用脚本形式：xsync  文件名称

```powershell
[root@hadoop102 bin]$ xsync /home/root/bin
```

## 2.3 集群配置

1. 集群部署规划

|      | Hadoop100          | Hadoop101                    | Hadoop102                   |
| ---- | ------------------ | ---------------------------- | --------------------------- |
| HDFS | NameNode  DataNode | DataNode                     | SecondaryNameNode  DataNode |
| YARN | NodeManager        | ResourceManager  NodeManager | NodeManager                 |

2. 配置集群

（1）配置core-site.xml

```powershell
[root@hadoop100 hadoop]$ vi core-site.xml
```

在该文件中编写如下配置

```xml
<!-- 指定HDFS中NameNode的地址 -->
<property>
   <name>fs.defaultFS</name>
   <value>hdfs://hadoop100:9000</value>
</property>
<!-- 指定Hadoop运行时产生文件的存储目录 -->
<property>
  <name>hadoop.tmp.dir</name>
  <value>/opt/module/hadoop-2.7.2/data/tmp</value>
</property>
```

（2）HDFS配置文件

配置hadoop-env.sh

```powershell
[root@hadoop100 hadoop]$ vi hadoop-env.sh
# 修改文件
export JAVA_HOME=/opt/module/jdk1.8.0_144
```

配置hdfs-site.xml

```powershell
[root@hadoop102 hadoop]$ vi hdfs-site.xml
```

在该文件中编写如下配置

```xml
<property>
  <name>dfs.replication</name>
  <value>3</value>
</property>
<!-- 指定Hadoop辅助名称节点主机配置 -->
<property>
   <name>dfs.namenode.secondary.http-address</name>
   <value>hadoop102:50090</value>
</property>
```

（3）YARN配置文件

配置yarn-site.xml

```
[root@hadoop102 hadoop]$ vi yarn-site.xml
```

在该文件中增加如下配置

```xml
<!-- Reducer获取数据的方式 -->
<property>
  <name>yarn.nodemanager.aux-services</name>
  <value>mapreduce_shuffle</value>
</property>
<!-- 指定YARN的ResourceManager的地址 -->
<property>
  <name>yarn.resourcemanager.hostname</name>
  <value>hadoop101</value>
</property>
```

（4）MapReduce配置文件

配置mapred-env.sh

```
[root@hadoop100 hadoop]$ vi mapred-env.sh

# 修改文件
export JAVA_HOME=/opt/module/jdk1.8.0_144
```

配置mapred-site.xml

```
[root@hadoop100 hadoop]$ cp mapred-site.xml.template mapred-site.xml
[root@hadoop100 hadoop]$ vi mapred-site.xml
```

在该文件中增加如下配置

```xml
<!-- 指定MR运行在Yarn上 -->
<property>
 <name>mapreduce.framework.name</name>
  <value>yarn</value>
</property>
```

3．在集群上分发配置好的Hadoop配置文件

```
[root@hadoop100 hadoop]$ xsync /opt/module/hadoop-2.7.2/
```

4．查看文件分发情况

```
[root@hadoop101 hadoop]$ cat /opt/module/hadoop-2.7.2/etc/hadoop/core-site.xml
```

## 2.4 群起集群

1. 配置slaves

```
[root@hadoop102 hadoop]$ vi /opt/module/hadoop-2.7.2/etc/hadoop/slaves
```

在该文件中增加如下内容：

```
hadoop100
hadoop101
hadoop102
```

注意：该文件中添加的内容结尾不允许有空格，文件中不允许有空行。

同步所有节点配置文件

```
[root@hadoop100 hadoop]$ xsync /opt/module/hadoop-2.7.2/etc/hadoop/slaves
```

2. 启动集群

（1）如果集群是第一次启动，需要格式化NameNode（注意格式化之前，一定要先停止上次启动的所有namenode和datanode进程，然后再删除data和log数据）

```
[root@hadoop102 hadoop-2.7.2]$ bin/hdfs namenode -format
```

（2）启动HDFS

```
[root@hadoop100 hadoop-2.7.2]$ sbin/start-dfs.sh
[root@hadoop100 hadoop-2.7.2]$ jps
4166 NameNode
4482 Jps
4263 DataNode
[root@hadoop101 hadoop-2.7.2]$ jps
3218 DataNode
3288 Jps
[root@hadoop102 hadoop-2.7.2]$ jps
3221 DataNode
3283 SecondaryNameNode
3364 Jps
```

（3）启动YARN

```
[root@hadoop101 hadoop-2.7.2]$ sbin/start-yarn.sh
```

注意：NameNode和ResourceManger如果不是同一台机器，不能在NameNode上启动 YARN，应该在ResouceManager所在的机器上启动YARN。

（4）Web端查看SecondaryNameNode

本机浏览器中输入：http://hadoop100:50090/status.html注意需要配置主机http服务，并且关闭了防火墙。在本机需要配置hosts主机名，如果没有配置，则直接写ip。

3. 集群基本测试

（1）上传文件到集群

```
[root@hadoop100 hadoop-2.7.2]$ hdfs dfs -mkdir -p /user/root/input
[root@hadoop100 hadoop-2.7.2]$ hdfs dfs -put wcinput/wc.input /user/root/input
```

（2）上传文件后查看文件存放在什么位置

（a）查看HDFS文件存储路径

```
[root@hadoop100 subdir0]$ pwd
/opt/module/hadoop-2.7.2/data/tmp/dfs/data/current/BP-938951106-192.168.10.107-1495462844069/current/finalized/subdir0/subdir0
```

（b）查看HDFS在磁盘存储文件内容

```
[root@hadoop100 subdir0]$ cat blk_1073741825
hadoop yarn
hadoop mapreduce 
root
root
```

（3）下载

```
[root@hadoop100 hadoop-2.7.2]$ bin/hadoop fs -get /user/root/input/hadoop-2.7.2.tar.gz ./
```