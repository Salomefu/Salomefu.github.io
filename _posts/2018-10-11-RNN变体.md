---
layout: post
title: RNN变体
subtitle: 
date: 2018-10-11
author: Salome
header-img: img/post-bg-2015.jpg
catalog: true
tags:
    - machine learning
---

# 1. intuition

这里首先从简单RNN的长时间步会导致的梯度问题说起，进而引出了RNN的变体网络，如GRU和LSTM。针对RNN只能接受过去的信息而不能看到未来又提出了BI-LSTM.  有人说为什么会是这种结构？首先对每个网络结构，每个人都能根据自己灵感去改造网络，我想那些研究员也是做过很多尝试然后发现一个不错的网络，为什么效果好，nobody knows。所以我觉得RNN掌握最基本的结构是如何实现的就可以了，这些变体了解其提出背景和使用场景，相信未来也会有无数的变体被提出。



# 2 class notes

## 2.1 循环神经网络的梯度消失（Vanishing gradients with **RNN**s）

你已经了解了**RNN**时如何工作的了，并且知道如何应用到具体问题上，比如命名实体识别，比如语言模型，你也看到了怎么把反向传播用于**RNN**。其实，基本的**RNN**算法还有一个很大的问题，就是梯度消失的问题。这节课我们会讨论，在下几节课我们会讨论一些方法用来解决这个问题。

![](https://tva1.sinaimg.cn/large/006y8mN6gy1g98yo9l3ohj30go0a2wg7.jpg)

你已经知道了**RNN**的样子，现在我们举个语言模型的例子，假如看到这个句子（上图编号1所示），“**The cat, which already ate ……, was full.**”，前后应该保持一致，因为**cat**是单数，所以应该用**was**。“**The cats, which ate ……, were full.**”（上图编号2所示），**cats**是复数，所以用**were**。这个例子中的句子有长期的依赖，最前面的单词对句子后面的单词有影响。但是我们目前见到的基本的**RNN**模型（上图编号3所示的网络模型），不擅长捕获这种长期依赖效应，解释一下为什么。

你应该还记得之前讨论的训练很深的网络，我们讨论了梯度消失的问题。比如说一个很深很深的网络（上图编号4所示），100层，甚至更深，对这个网络从左到右做前向传播然后再反向传播。我们知道如果这是个很深的神经网络，从输出$\hat y$得到的梯度很难传播回去，很难影响靠前层的权重，很难影响前面层（编号5所示的层）的计算。

对于有同样问题的**RNN**，首先从左到右前向传播，然后反向传播。但是反向传播会很困难，因为同样的梯度消失的问题，后面层的输出误差（上图编号6所示）很难影响前面层（上图编号7所示的层）的计算。这就意味着，实际上很难让一个神经网络能够意识到它要记住看到的是单数名词还是复数名词，然后在序列后面生成依赖单复数形式的**was**或者**were**。而且在英语里面，这中间的内容（上图编号8所示）可以任意长，对吧？所以你需要长时间记住单词是单数还是复数，这样后面的句子才能用到这些信息。也正是这个原因，所以基本的**RNN**模型会有很多局部影响，意味着这个输出$\hat y^{<3>}$（上图编号9所示）主要受$\hat y^{<3>}$附近的值（上图编号10所示）的影响，上图编号11所示的一个数值主要与附近的输入（上图编号12所示）有关，上图编号6所示的输出，基本上很难受到序列靠前的输入（上图编号10所示）的影响，这是因为不管输出是什么，不管是对的，还是错的，这个区域都很难反向传播到序列的前面部分，也因此网络很难调整序列前面的计算。这是基本的**RNN**算法的一个缺点，我们会在下几节视频里处理这个问题。如果不管的话，**RNN**会不擅长处理长期依赖的问题。

![](https://tva1.sinaimg.cn/large/006y8mN6gy1g98yod9yxxj30go01cweg.jpg)

尽管我们一直在讨论梯度消失问题，但是，你应该记得我们在讲很深的神经网络时，我们也提到了梯度爆炸，我们在反向传播的时候，随着层数的增多，梯度不仅可能指数型的下降，也可能指数型的上升。事实上梯度消失在训练**RNN**时是首要的问题，尽管梯度爆炸也是会出现，但是梯度爆炸很明显，因为指数级大的梯度会让你的参数变得极其大，以至于你的网络参数崩溃。所以梯度爆炸很容易发现，因为参数会大到崩溃，你会看到很多**NaN**，或者不是数字的情况，这意味着你的网络计算出现了数值溢出。如果你发现了梯度爆炸的问题，一个解决方法就是用梯度修剪。梯度修剪的意思就是观察你的梯度向量，如果它大于某个阈值，缩放梯度向量，保证它不会太大，这就是通过一些最大值来修剪的方法。所以如果你遇到了梯度爆炸，如果导数值很大，或者出现了**NaN**，就用梯度修剪，这是相对比较鲁棒的，这是梯度爆炸的解决方法。然而梯度消失更难解决，这也是我们下几节视频的主题。

总结一下，在前面的课程，我们了解了训练很深的神经网络时，随着层数的增加，导数有可能指数型的下降或者指数型的增加，我们可能会遇到梯度消失或者梯度爆炸的问题。加入一个**RNN**处理1,000个时间序列的数据集或者10,000个时间序列的数据集，这就是一个1,000层或者10,000层的神经网络，这样的网络就会遇到上述类型的问题。梯度爆炸基本上用梯度修剪就可以应对，但梯度消失比较棘手。我们下节会介绍**GRU**，门控循环单元网络，这个网络可以有效地解决梯度消失的问题，并且能够使你的神经网络捕获更长的长期依赖，我们去下个视频一探究竟吧。

## 2.2 **GRU**单元（Gated Recurrent Unit（**GRU**））

你已经了解了基础的**RNN**模型的运行机制，在本节视频中你将会学习门控循环单元，它改变了**RNN**的隐藏层，使其可以更好地捕捉深层连接，并改善了梯度消失问题，让我们看一看。

![](https://tva1.sinaimg.cn/large/006y8mN6gy1g98yoeqd48j30go07y3yx.jpg)

你已经见过了这个公式，$a^{< t >} = g(W_{a}\left\lbrack a^{< t - 1 >},x^{< t >}\right\rbrack +b_{a})$，在**RNN**的时间$t$处，计算激活值。我把这个画个图，把**RNN**的单元画个图，画一个方框，输入$a^{<t-1>}$（上图编号1所示），即上一个时间步的激活值，再输入$x^{<t>}$（上图编号2所示），再把这两个并起来，然后乘上权重项，在这个线性计算之后（上图编号3所示），如果$g$是一个**tanh**激活函数，再经过**tanh**计算之后，它会计算出激活值$a^{<t>}$。然后激活值$a^{<t>}$将会传**softmax**单元（上图编号4所示），或者其他用于产生输出$y^{<t>}$的东西。就这张图而言，这就是**RNN**隐藏层的单元的可视化呈现。我向展示这张图，因为我们将使用相似的图来讲解门控循环单元。![1521560729](https://tva1.sinaimg.cn/large/006y8mN6gy1g98yo94eoqj30go07imxh.jpg)



![](https://tva1.sinaimg.cn/large/006y8mN6gy1g98yodnagzj30go024q36.jpg)

许多**GRU**的想法都来分别自于**Yu Young Chang, Kagawa，Gaza Hera, Chang Hung Chu**和
**Jose Banjo**的两篇论文。我再引用上个视频中你已经见过的这个句子，“**The cat, which already ate……, was full.**”，你需要记得猫是单数的，为了确保你已经理解了为什么这里是**was**而不是**were**，“**The cat was full.**”或者是“**The cats were full**”。当我们从左到右读这个句子，**GRU**单元将会有个新的变量称为$c$，代表细胞（**cell**），即记忆细胞（下图编号1所示）。记忆细胞的作用是提供了记忆的能力，比如说一只猫是单数还是复数，所以当它看到之后的句子的时候，它仍能够判断句子的主语是单数还是复数。于是在时间$t$处，有记忆细胞$c^{<t>}$，然后我们看的是，**GRU**实际上输出了激活值$a^{<t>}$，$c^{<t>} = a^{<t>}$（下图编号2所示）。于是我们想要使用不同的符号$c$和$a$来表示记忆细胞的值和输出的激活值，即使它们是一样的。我现在使用这个标记是因为当我们等会说到**LSTMs**的时候，这两个会是不同的值，但是现在对于**GRU**，$c^{<t>}$的值等于$a^{<t>}$的激活值。

所以这些等式表示了**GRU**单元的计算，在每个时间步，我们将用一个候选值重写记忆细胞，即${\tilde{c}}^{<t>}$的值，所以它就是个候选值，替代了$c^{<t>}$的值。然后我们用**tanh**激活函数来计算，${\tilde{c}}^{<t>} =tanh(W_{c}\left\lbrack c^{<t-1>},x^{<t>} \right\rbrack +b_{c})$，所以${\tilde{c}}^{<t>}$的值就是个替代值，代替表示$c^{<t>}$的值（下图编号3所示）。

![](https://tva1.sinaimg.cn/large/006y8mN6gy1g98yoai6g2j30go084q3k.jpg)

重点来了，在**GRU**中真正重要的思想是我们有一个门，我先把这个门叫做$\Gamma_{u}$（上图编号4所示），这是个下标为$u$的大写希腊字母$\Gamma$，$u$代表更新门，这是一个0到1之间的值。为了让你直观思考**GRU**的工作机制，先思考$\Gamma_{u}$，这个一直在0到1之间的门值，实际上这个值是把这个式子带入**sigmoid**函数得到的，$\Gamma_{u}= \sigma(W_{u}\left\lbrack c^{<t-1>},x^{<t>} \right\rbrack +b_{u})$。我们还记得**sigmoid**函数是上图编号5所示这样的，它的输出值总是在0到1之间，对于大多数可能的输入，**sigmoid**函数的输出总是非常接近0或者非常接近1。在这样的直觉下，可以想到$\Gamma_{u}$在大多数的情况下非常接近0或1。然后这个字母**u**表示“**update**”，我选了字母$\Gamma$是因为它看起来像门。还有希腊字母**G**，**G**是门的首字母，所以**G**表示门。

然后**GRU**的关键部分就是上图编号3所示的等式，我们刚才写出来的用$\tilde{c}$更新$c$的等式。然后门决定是否要真的更新它。于是我们这么看待它，记忆细胞$c^{<t>}$将被设定为0或者1，这取决于你考虑的单词在句子中是单数还是复数，因为这里是单数情况，所以我们先假定它被设为了1，或者如果是复数的情况我们就把它设为0。然后**GRU**单元将会一直记住$c^{<t>}$的值，直到上图编号7所示的位置，$c^{<t>}$的值还是1，这就告诉它，噢，这是单数，所以我们用**was**。于是门，即$\Gamma_{u}$的作用就是决定什么时候你会更新这个值，特别是当你看到词组**the cat**，即句子的主语猫，这就是一个好时机去更新这个值。然后当你使用完它的时候，“**The cat, which already ate……, was full.**”，然后你就知道，我不需要记住它了，我可以忘记它了。

![](https://tva1.sinaimg.cn/large/006y8mN6gy1g98yo8j05xj30go098ab3.jpg)

所以我们接下来要给**GRU**用的式子就是$c^{<t>} = \Gamma_{u}*{\tilde{c}}^{<t>} +\left( 1- \Gamma_{u} \right)*c^{<t-1>}$（上图编号1所示）。你应该注意到了，如果这个更新值$\Gamma_{u} =1$，也就是说把这个新值，即$c^{<t>}$设为候选值（$\Gamma_{u} =1$时简化上式，$c^{<t>} = {\tilde{c}}^{<t>}$）。将门值设为1（上图编号2所示），然后往前再更新这个值。对于所有在这中间的值，你应该把门的值设为0，即$\Gamma_{u}= 0$，意思就是说不更新它，就用旧的值。因为如果$\Gamma_{u} = 0$，则$c^{<t>} =c^{<t-1>}$，$c^{<t>}$等于旧的值。甚至你从左到右扫描这个句子，当门值为0的时候（上图编号3所示，中间$\Gamma_{u}=0$一直为0，表示一直不更新），就是说不更新它的时候，不要更新它，就用旧的值，也不要忘记这个值是什么，这样即使你一直处理句子到上图编号4所示，$c^{<t>}$应该会一直等$c^{<t-1>}$，于是它仍然记得猫是单数的。

让我再画个图来（下图所示）解释一下**GRU**单元，顺便说一下，当你在看网络上的博客或者教科书或者教程之类的，这些图对于解释**GRU**和我们稍后会讲的**LSTM**是相当流行的，我个人感觉式子在图片中比较容易理解，那么即使看不懂图片也没关系，我就画画，万一能帮得上忙就最好了。

**GRU**单元输入$c^{<t-1>}$（下图编号1所示），对于上一个时间步，先假设它正好等于$a^{<t-1>}$，所以把这个作为输入。然后$x^{<t>}$也作为输入（下图编号2所示），然后把这两个用合适权重结合在一起，再用**tanh**计算，算出${\tilde{c}}^{<t>}$，${\tilde{c}}^{<t>} =tanh(W_{c}\left\lbrack c^{<t-1>},x^{<t>} \right\rbrack +b_{c})$，即$c^{<t>}$的替代值。

再用一个不同的参数集，通过**sigmoid**激活函数算出$\Gamma_{u}$，$\Gamma_{u}= \sigma(W_{u}\left\lbrack c^{<t-1>},x^{<t>} \right\rbrack +b_{u})$，即更新门。最后所有的值通过另一个运算符结合，我并不会写出公式，但是我用紫色阴影标注的这个方框（下图编号5所示，其所代表的运算过程即下图编号13所示的等式），代表了这个式子。所以这就是紫色运算符所表示的是，它输入一个门值（下图编号6所示），新的候选值（下图编号7所示），这再有一个门值（下图编号8所示）和$c^{<t>}$的旧值（下图编号9所示），所以它把这个（下图编号1所示）、这个（下图编号3所示）和这个（下图编号4所示）作为输入一起产生记忆细胞的新值$c^{<t>}$，所以$c^{<t>}$等于$a^{<t>}$。如果你想，你也可以也把这个代入**softmax**或者其他预测$y^{<t>}$的东西。

![](https://tva1.sinaimg.cn/large/006y8mN6gy1g98yocrglfj30go096wfz.jpg)

这就是**GRU**单元或者说是一个简化过的**GRU**单元，它的优点就是通过门决定，当你从左（上图编号10所示）到右扫描一个句子的时候，这个时机是要更新某个记忆细胞，还是不更新，不更新（上图编号11所示，中间$\Gamma_{u}=0$一直为0，表示一直不更新）直到你到你真的需要使用记忆细胞的时候（上图编号12所示），这可能在句子之前就决定了。因为sigmoid的值，现在因为门很容易取到0值，只要这个值是一个很大的负数，再由于数值上的四舍五入，上面这些门大体上就是0，或者说非常非常非常接近0。所以在这样的情况下，这个更新式子（上图编号13所示的等式）就会变成$c^{<t>} = c^{<t-1>}$，这非常有利于维持细胞的值。因为$\Gamma_{u}$很接近0，可能是0.000001或者更小，这就不会有梯度消失的问题了。因为$\Gamma_{u}$很接近0，这就是说$c^{<t>}$几乎就等于$c^{<t-1>}$，而且$c^{<t>}$的值也很好地被维持了，即使经过很多很多的时间步（上图编号14所示）。这就是缓解梯度消失问题的关键，因此允许神经网络运行在非常庞大的依赖词上，比如说**cat**和**was**单词即使被中间的很多单词分割开。

![](https://tva1.sinaimg.cn/large/006y8mN6gy1g98yoaziqxj30go097q4h.jpg)

现在我想说下一些实现的细节，在这个我写下的式子中$c^{<t>}$可以是一个向量（上图编号1所示），如果你有100维的隐藏的激活值，那么$c^{<t>}$也是100维的，${\tilde{c}}^{<t>}$也是相同的维度（${\tilde{c}}^{<t>} =tanh(W_{c}\left\lbrack c^{<t-1>},x^{<t>} \right\rbrack +b_{c})$），$\Gamma_{u}$也是相同的维度（$\Gamma_{u}= \sigma(W_{u}\left\lbrack c^{<t-1>},x^{<t>} \right\rbrack +b_{u})$），还有画在框中的其他值。这样的话“\*”实际上就是元素对应的乘积（$c^{<t>} = \Gamma_{u}*{\tilde{c}}^{<t>} +\left( 1- \Gamma_{u} \right)*c^{<t-1>}$），所以这里的$\Gamma_{u}$：（$\Gamma_{u}= \sigma(W_{u}\left\lbrack c^{<t-1>},x^{<t>} \right\rbrack +b_{u})$），即如果门是一个100维的向量，$\Gamma_{u}$也就100维的向量，里面的值几乎都是0或者1，就是说这100维的记忆细胞$c^{<t>}$（$c^{<t>}=a^{<t>}$上图编号1所示）就是你要更新的比特。

当然在实际应用中$\Gamma_{u}$不会真的等于0或者1，有时候它是0到1的一个中间值（上图编号5所示），但是这对于直观思考是很方便的，就把它当成确切的0，完全确切的0或者就是确切的1。元素对应的乘积做的就是告诉**GRU**单元哪个记忆细胞的向量维度在每个时间步要做更新，所以你可以选择保存一些比特不变，而去更新其他的比特。比如说你可能需要一个比特来记忆猫是单数还是复数，其他比特来理解你正在谈论食物，因为你在谈论吃饭或者食物，然后你稍后可能就会谈论“**The cat was full.**”，你可以每个时间点只改变一些比特。

你现在已经理解**GRU**最重要的思想了，幻灯片中展示的实际上只是简化过的**GRU**单元，现在来描述一下完整的**GRU**单元。

对于完整的**GRU**单元我要做的一个改变就是在我们计算的第一个式子中给记忆细胞的新候选值加上一个新的项，我要添加一个门$\Gamma_{r}$（下图编号1所示），你可以认为$r$代表相关性（**relevance**）。这个$\Gamma_{r}$门告诉你计算出的下一个$c^{<t>}$的候选值${\tilde{c}}^{<t>}$跟$c^{<t-1>}$有多大的相关性。计算这个门$\Gamma_{r}$需要参数，正如你看到的这个，一个新的参数矩阵$W_{r}$，$\Gamma_{r}= \sigma(W_{r}\left\lbrack c^{<t-1>},x^{<t>} \right\rbrack + b_{r})$。

![](https://tva1.sinaimg.cn/large/006y8mN6gy1g98yof1vw2j30go0a6mxu.jpg)

正如你所见，有很多方法可以来设计这些类型的神经网络，然后我们为什么有$\Gamma_{r}$？为什么不用上一张幻灯片里的简单的版本？这是因为多年来研究者们试验过很多很多不同可能的方法来设计这些单元，去尝试让神经网络有更深层的连接，去尝试产生更大范围的影响，还有解决梯度消失的问题，**GRU**就是其中一个研究者们最常使用的版本，也被发现在很多不同的问题上也是非常健壮和实用的。你可以尝试发明新版本的单元，只要你愿意。但是**GRU**是一个标准版本，也就是最常使用的。你可以想象到研究者们也尝试了很多其他版本，类似这样的但不完全是，比如我这里写的这个。然后另一个常用的版本被称为**LSTM**，表示长短时记忆网络，这个我们会在下节视频中讲到，但是**GRU**和**LSTM**是在神经网络结构中最常用的两个具体实例。

还有在符号上的一点，我尝试去定义固定的符号让这些概念容易理解，如果你看学术文章的话，你有的时候会看到有些人使用另一种符号$\tilde{x}$，$u$，$r$和$h$表示这些量。但我试着在**GRU**和**LSTM**之间用一种更固定的符号，比如使用更固定的符号$\Gamma$来表示门，所以希望这能让这些概念更好理解。

所以这就是**GRU**，即门控循环单元，这是**RNN**的其中之一。这个结构可以更好捕捉非常长范围的依赖，让**RNN**更加有效。然后我简单提一下其他常用的神经网络，比较经典的是这个叫做**LSTM**，即长短时记忆网络，我们在下节视频中讲解。

（**Chung J, Gulcehre C, Cho K H, et al. Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling[J]. Eprint Arxiv, 2014.**

**Cho K, Merrienboer B V, Bahdanau D, et al. On the Properties of Neural Machine Translation: Encoder-Decoder Approaches[J]. Computer Science, 2014.**）

## 2.3 长短期记忆（**LSTM**（long short term memory）unit）

在上一个视频中你已经学了**GRU**（门控循环单元）。它能够让你可以在序列中学习非常深的连接。其他类型的单元也可以让你做到这个，比如**LSTM**即长短时记忆网络，甚至比**GRU**更加有效，让我们看看。

![](https://tva1.sinaimg.cn/large/006y8mN6gy1g98yobwkh4j308c060glu.jpg)

这里是上个视频中的式子，对于**GRU**我们有$a^{< t >} = c^{<t>}$。

还有两个门:

更新门$\Gamma_{u}$（**the update gate**）

相关门$\Gamma_{r}$（**the relevance gate**）

${\tilde{c}}^{<t>}$，这是代替记忆细胞的候选值，然后我们使用更新门$\Gamma_{u}$来决定是否要用${\tilde{c}}^{<t>}$ 更新$c^{<t>}$。

**LSTM**是一个比**GRU**更加强大和通用的版本，这多亏了 **Sepp Hochreiter**和 **Jurgen Schmidhuber**，感谢那篇开创性的论文，它在序列模型上有着巨大影响。我感觉这篇论文是挺难读懂的，虽然我认为这篇论文在深度学习社群有着重大的影响，它深入讨论了梯度消失的理论，我感觉大部分的人学到**LSTM**的细节是在其他的地方，而不是这篇论文。

![](https://tva1.sinaimg.cn/large/006y8mN6gy1g98yoa1ggdj30go096gmv.jpg)

这就是**LSTM**主要的式子（上图编号2所示），我们继续回到记忆细胞**c**上面来，使用${\tilde{c}}^{<t>} = tanh(W_{c}\left\lbrack a^{<t-1>},x^{<t>} \right\rbrack +b_{c}$来更新它的候选值${\tilde{c}}^{<t>}$（上图编号3所示）。注意了，在**LSTM**中我们不再有$a^{<t>} = c^{<t>}$的情况，这是现在我们用的是类似于左边这个式子（上图编号4所示），但是有一些改变，现在我们专门使用$a^{<t>}$或者$a^{<t-1>}$，而不是用$c^{<t-1>}$，我们也不用$\Gamma_{r}$，即相关门。虽然你可以使用**LSTM**的变体，然后把这些东西（左边所示的**GRU**公式）都放回来，但是在更加典型的**LSTM**里面，我们先不那样做。

我们像以前那样有一个更新门$\Gamma_{u}$和表示更新的参数$W_{u}$，$\Gamma_{u}= \sigma(W_{u}\left\lbrack a^{<t-1>},x^{<t>} \right\rbrack +b_{u})$（上图编号5所示）。一个**LSTM**的新特性是不只有一个更新门控制，这里的这两项（上图编号6，7所示），我们将用不同的项来代替它们，要用别的项来取代$\Gamma_{u}$和$1-\Gamma_{u}$，这里（上图编号6所示）我们用$\Gamma_{u}$。

然后这里（上图编号7所示）用遗忘门（**the forget gate**），我们叫它$\Gamma_{f}$，所以这个$\Gamma_{f} =\sigma(W_{f}\left\lbrack a^{<t-1>},x^{<t>} \right\rbrack +b_{f})$（上图编号8所示）；

然后我们有一个新的输出门，$\Gamma_{o} =\sigma(W_{o}\left\lbrack a^{<t-1>},x^{<t>} \right\rbrack +>b_{o})$（上图编号9所示）；

于是记忆细胞的更新值$c^{<t>} =\Gamma_{u}*{\tilde{c}}^{<t>} + \Gamma_{f}*c^{<t-1>}$（上图编号10所示）；

所以这给了记忆细胞选择权去维持旧的值$c^{<t-1>}$或者就加上新的值${\tilde{c}}^{<t>}$，所以这里用了单独的更新门$\Gamma_{u}$和遗忘门$\Gamma_{f}$，

然后这个表示更新门（$\Gamma_{u}= \sigma(W_{u}\left\lbrack a^{<t-1>},x^{<t>} \right\rbrack +b_{u})$上图编号5所示）；

遗忘门（$\Gamma_{f} =\sigma(W_{f}\left\lbrack a^{<t-1>},x^{<t>} \right\rbrack +b_{f})$上图编号8所示）和输出门（上图编号9所示）。

最后$a^{<t>} = c^{<t>}$的式子会变成$a^{<t>} = \Gamma_{o}*c^{<t>}$。这就是**LSTM**主要的式子了，然后这里（上图编号11所示）有三个门而不是两个，这有点复杂，它把门放到了和之前有点不同的地方。

![](https://tva1.sinaimg.cn/large/006y8mN6gy1g98yo7k31tj30go09awg2.jpg)

再提一下，这些式子就是控制**LSTM**行为的主要的式子了（上图编号1所示）。像之前一样用图片稍微解释一下，先让我把图画在这里（上图编号2所示）。如果图片过于复杂，别担心，我个人感觉式子比图片好理解，但是我画图只是因为它比较直观。这个右上角的图的灵感来自于**Chris Ola**的一篇博客，标题是《理解**LSTM**网络》（**Understanding LSTM Network**），这里的这张图跟他博客上的图是很相似的，但关键的不同可能是这里的这张图用了$a^{<t-1>}$和$x^{<t>}$来计算所有门值（上图编号3，4所示），在这张图里是用$a^{<t-1>}$， $x^{<t>}$一起来计算遗忘门$\Gamma_{f}$的值，还有更新门$\Gamma_{u}$以及输出门$\Gamma_{o}$（上图编号4所示）。然后它们也经过**tanh**函数来计算${\tilde{c}}^{<t>}$（上图编号5所示），这些值被用复杂的方式组合在一起，比如说元素对应的乘积或者其他的方式来从之前的$c^{<t-1>}$（上图编号6所示）中获得$c^{<t>}$（上图编号7所示）。

这里其中一个元素很有意思，如你在这一堆图（上图编号8所示的一系列图片）中看到的，这是其中一个，再把他们连起来，就是把它们按时间次序连起来，这里（上图编号9所示）输入$x^{<1>}$，然后$x^{<2>}$，$x^{<3>}$，然后你可以把这些单元依次连起来，这里输出了上一个时间的$a$，$a$会作为下一个时间步的输入，$c$同理。在下面这一块，我把图简化了一下（相对上图编号2所示的图有所简化）。然后这有个有意思的事情，你会注意到上面这里有条线（上图编号10所示的线），这条线显示了只要你正确地设置了遗忘门和更新门，**LSTM**是相当容易把$c^{<0>}$的值（上图编号11所示）一直往下传递到右边，比如$c^{<3>} = c^{<0>}$（上图编号12所示）。这就是为什么**LSTM**和**GRU**非常擅长于长时间记忆某个值，对于存在记忆细胞中的某个值，即使经过很长很长的时间步。

这就是**LSTM**，你可能会想到这里和一般使用的版本会有些不同，最常用的版本可能是门值不仅取决于$a^{<t-1>}$和$x^{<t>}$，有时候也可以偷窥一下$c^{<t-1>}$的值（上图编号13所示），这叫做“窥视孔连接”（**peephole connection**）。虽然不是个好听的名字，但是你想，“**偷窥孔连接**”其实意思就是门值不仅取决于$a^{<t-1>}$和$x^{<t>}$，也取决于上一个记忆细胞的值（$c^{<t-1>}$），然后“偷窥孔连接”就可以结合这三个门（$\Gamma_{u}$、$\Gamma_{f}$、$\Gamma_{o}$）来计算了。

如你所见**LSTM**主要的区别在于一个技术上的细节，比如这（上图编号13所示）有一个100维的向量，你有一个100维的隐藏的记忆细胞单元，然后比如第50个$c^{<t-1>}$的元素只会影响第50个元素对应的那个门，所以关系是一对一的，于是并不是任意这100维的$c^{<t-1>}$可以影响所有的门元素。相反的，第一个$c^{<t-1>}$的元素只能影响门的第一个元素，第二个元素影响对应的第二个元素，如此类推。但如果你读过论文，见人讨论“**偷窥孔连接**”，那就是在说$c^{<t-1>}$也能影响门值。

**LSTM**前向传播图：

![ST](https://tva1.sinaimg.cn/large/006y8mN6gy1g98yobh4vvj30go07ogm9.jpg)

![STM_rn](https://tva1.sinaimg.cn/large/006y8mN6gy1g98yocasrnj30go03ymxj.jpg)



**LSTM**反向传播计算：

**门求偏导：**

$d \Gamma_o^{\langle t \rangle} = da_{next}*\tanh(c_{next}) * \Gamma_o^{\langle t \rangle}*(1-\Gamma_o^{\langle t \rangle})\tag{1}$

$d\tilde c^{\langle t \rangle} = dc_{next}*\Gamma_i^{\langle t \rangle}+ \Gamma_o^{\langle t \rangle} (1-\tanh(c_{next})^2) * i_t * da_{next} * \tilde c^{\langle t \rangle} * (1-\tanh(\tilde c)^2) \tag{2}$

$d\Gamma_u^{\langle t \rangle} = dc_{next}*\tilde c^{\langle t \rangle} + \Gamma_o^{\langle t \rangle} (1-\tanh(c_{next})^2) * \tilde c^{\langle t \rangle} * da_{next}*\Gamma_u^{\langle t \rangle}*(1-\Gamma_u^{\langle t \rangle})\tag{3}$

$d\Gamma_f^{\langle t \rangle} = dc_{next}*\tilde c_{prev} + \Gamma_o^{\langle t \rangle} (1-\tanh(c_{next})^2) * c_{prev} * da_{next}*\Gamma_f^{\langle t \rangle}*(1-\Gamma_f^{\langle t \rangle})\tag{4}$

**参数求偏导 ：**

$ dW_f = d\Gamma_f^{\langle t \rangle} * \begin{pmatrix} a_{prev} \\ x_t\end{pmatrix}^T \tag{5} $
$ dW_u = d\Gamma_u^{\langle t \rangle} * \begin{pmatrix} a_{prev} \\ x_t\end{pmatrix}^T \tag{6} $
 $ dW_c = d\tilde c^{\langle t \rangle} * \begin{pmatrix} a_{prev} \\ x_t\end{pmatrix}^T \tag{7} $
$ dW_o = d\Gamma_o^{\langle t \rangle} * \begin{pmatrix} a_{prev} \\ x_t\end{pmatrix}^T \tag{8}$

为了计算$db_f, db_u, db_c, db_o$ 需要各自对$d\Gamma_f^{\langle t \rangle}, d\Gamma_u^{\langle t \rangle}, d\tilde c^{\langle t \rangle}, d\Gamma_o^{\langle t \rangle}$ 求和。

最后，计算隐藏状态、记忆状态和输入的偏导数：

$ da_{prev} = W_f^T*d\Gamma_f^{\langle t \rangle} + W_u^T * d\Gamma_u^{\langle t \rangle}+ W_c^T * d\tilde c^{\langle t \rangle} + W_o^T * d\Gamma_o^{\langle t \rangle} \tag{9}$

$ dc_{prev} = dc_{next}\Gamma_f^{\langle t \rangle} + \Gamma_o^{\langle t \rangle} * (1- \tanh(c_{next})^2)*\Gamma_f^{\langle t \rangle}*da_{next} \tag{10}$
$ dx^{\langle t \rangle} = W_f^T*d\Gamma_f^{\langle t \rangle} + W_u^T * d\Gamma_u^{\langle t \rangle}+ W_c^T * d\tilde c_t + W_o^T * d\Gamma_o^{\langle t \rangle}\tag{11} $


这就是**LSTM**，我们什么时候应该用**GRU**？什么时候用**LSTM**？这里没有统一的准则。而且即使我先讲解了**GRU**，在深度学习的历史上，**LSTM**也是更早出现的，而**GRU**是最近才发明出来的，它可能源于**Pavia**在更加复杂的**LSTM**模型中做出的简化。研究者们在很多不同问题上尝试了这两种模型，看看在不同的问题不同的算法中哪个模型更好，所以这不是个学术和高深的算法，我才想要把这两个模型展示给你。

**GRU**的优点是这是个更加简单的模型，所以更容易创建一个更大的网络，而且它只有两个门，在计算性上也运行得更快，然后它可以扩大模型的规模。

但是**LSTM**更加强大和灵活，因为它有三个门而不是两个。如果你想选一个使用，我认为**LSTM**在历史进程上是个更优先的选择，所以如果你必须选一个，我感觉今天大部分的人还是会把**LSTM**作为默认的选择来尝试。虽然我认为最近几年**GRU**获得了很多支持，而且我感觉越来越多的团队也正在使用**GRU**，因为它更加简单，而且还效果还不错，它更容易适应规模更加大的问题。

所以这就是**LSTM**，无论是**GRU**还是**LSTM**，你都可以用它们来构建捕获更加深层连接的神经网络。

（**Hochreiter S, Schmidhuber J. Long Short-Term Memory[J]. Neural Computation, 1997, 9(8):1735-1780.**）

## 2.4 双向循环神经网络（Bidirectional **RNN**）

现在，你已经了解了大部分**RNN**模型的关键的构件，还有两个方法可以让你构建更好的模型，其中之一就是双向**RNN**模型，这个模型可以让你在序列的某点处不仅可以获取之前的信息，还可以获取未来的信息，我们会在这个视频里讲解。第二个就是深层的**RNN**，我们会在下个视频里见到，现在先从双向**RNN**开始吧。

![](https://tva1.sinaimg.cn/large/006y8mN6gy1g98yo829dbj30go08igmo.jpg)

为了了解双向**RNN**的动机，我们先看一下之前在命名实体识别中已经见过多次的神经网络。这个网络有一个问题，在判断第三个词**Teddy**（上图编号1所示）是不是人名的一部分时，光看句子前面部分是不够的，为了判断$\hat y^{<3>}$（上图编号2所示）是0还是1，除了前3个单词，你还需要更多的信息，因为根据前3个单词无法判断他们说的是**Teddy熊**，还是前美国总统**Teddy Roosevelt**，所以这是一个非双向的或者说只有前向的**RNN**。我刚才所说的总是成立的，不管这些单元（上图编号3所示）是标准的**RNN**块，还是**GRU**单元或者是**LSTM**单元，只要这些构件都是只有前向的。

那么一个双向的**RNN**是如何解决这个问题的？下面解释双向**RNN**的工作原理。为了简单，我们用四个输入或者说一个只有4个单词的句子，这样输入只有4个，$x^{<1>}$到$x^{<4>}$。从这里开始的这个网络会有一个前向的循环单元叫做${\overrightarrow{a}}^{<1>}$，${\overrightarrow{a}}^{<2>}$，${\overrightarrow{a}}^{<3>}$还有${\overrightarrow{a}}^{<4>}$，我在这上面加个向右的箭头来表示前向的循环单元，并且他们这样连接（下图编号1所示）。这四个循环单元都有一个当前输入$x$输入进去，得到预测的$\hat y^{<1>}$，$\hat y^{<2>}$，$\hat y^{<3>}$和$\hat y^{<4>}$。

到目前为止，我还没做什么，仅仅是把前面幻灯片里的**RNN**画在了这里，只是在这些地方画上了箭头。我之所以在这些地方画上了箭头是因为我们想要增加一个反向循环层，这里有个${\overleftarrow{a}}^{<1>}$，左箭头代表反向连接，${\overleftarrow{a}}^{<2>}$反向连接，${\overleftarrow{a}}^{<3>}$反向连接，${\overleftarrow{a}}^{<4>}$反向连接，所以这里的左箭头代表反向连接。

![](https://tva1.sinaimg.cn/large/006y8mN6gy1g98yoe6obdj30go08sgmg.jpg)

同样，我们把网络这样向上连接，这个$a$反向连接就依次反向向前连接（上图编号2所示）。这样，这个网络就构成了一个无环图。给定一个输入序列$x^{<1>}$到$x^{<4>}$，这个序列首先计算前向的${\overrightarrow{a}}^{<1>}$，然后计算前向的${\overrightarrow{a}}^{<2>}$，接着${\overrightarrow{a}}^{<3>}$，${\overrightarrow{a}}^{<4>}$。而反向序列从计算${\overleftarrow{a}}^{<4>}$开始，反向进行，计算反向的${\overleftarrow{a}}^{<3>}$。你计算的是网络激活值，这不是反向而是前向的传播，而图中这个前向传播一部分计算是从左到右，一部分计算是从右到左。计算完了反向的${\overleftarrow{a}}^{<3>}$，可以用这些激活值计算反向的${\overleftarrow{a}}^{<2>}$，然后是反向的${\overleftarrow{a}}^{<1>}$，把所有这些激活值都计算完了就可以计算预测结果了。

![](https://tva1.sinaimg.cn/large/006y8mN6gy1g98yo73xy6j30go09hta3.jpg)

举个例子，为了预测结果，你的网络会有如$\hat y^{<t>}$，$\hat y^{<t>} =g(W_{g}\left\lbrack {\overrightarrow{a}}^{< t >},{\overleftarrow{a}}^{< t >} \right\rbrack +b_{y})$（上图编号1所示）。比如你要观察时间3这里的预测结果，信息从$x^{<1>}$过来，流经这里，前向的${\overrightarrow{a}}^{<1>}$到前向的${\overrightarrow{a}}^{<2>}$，这些函数里都有表达，到前向的${\overrightarrow{a}}^{<3>}$再到$\hat y^{<3>}$（上图编号2所示的路径），所以从$x^{<1>}$，$x^{<2>}$，$x^{<3>}$来的信息都会考虑在内，而从$x^{<4>}$来的信息会流过反向的${\overleftarrow{a}}^{<4>}$，到反向的${\overleftarrow{a}}^{<3>}$再到$\hat y^{<3>}$（上图编号3所示的路径）。这样使得时间3的预测结果不仅输入了过去的信息，还有现在的信息，这一步涉及了前向和反向的传播信息以及未来的信息。给定一个句子"**He said Teddy Roosevelt...**"来预测**Teddy**是不是人名的一部分，你需要同时考虑过去和未来的信息。

这就是双向循环神经网络，并且这些基本单元不仅仅是标准**RNN**单元，也可以是**GRU**单元或者**LSTM**单元。事实上，很多的**NLP**问题，对于大量有自然语言处理问题的文本，有**LSTM**单元的双向**RNN**模型是用的最多的。所以如果有**NLP**问题，并且文本句子都是完整的，首先需要标定这些句子，一个有**LSTM**单元的双向**RNN**模型，有前向和反向过程是一个不错的首选。

以上就是双向**RNN**的内容，这个改进的方法不仅能用于基本的**RNN**结构，也能用于**GRU**和**LSTM**。通过这些改变，你就可以用一个用**RNN**或**GRU**或**LSTM**构建的模型，并且能够预测任意位置，即使在句子的中间，因为模型能够考虑整个句子的信息。这个双向**RNN**网络模型的缺点就是你需要完整的数据的序列，你才能预测任意位置。比如说你要构建一个语音识别系统，那么双向**RNN**模型需要你考虑整个语音表达，但是如果直接用这个去实现的话，你需要等待这个人说完，然后获取整个语音表达才能处理这段语音，并进一步做语音识别。对于实际的语音识别的应用通常会有更加复杂的模块，而不是仅仅用我们见过的标准的双向**RNN**模型。但是对于很多自然语言处理的应用，如果你总是可以获取整个句子，这个标准的双向**RNN**算法实际上很高效。

好的，这就是双向**RNN**，下一个视频，也是这周的最后一个，我们会讨论如何用这些概念，标准的**RNN**，**LSTM**单元，**GRU**单元，还有双向的版本，构建更深的网络。

## 2.5 深层循环神经网络（Deep **RNN**s）

目前你学到的不同**RNN**的版本，每一个都可以独当一面。但是要学习非常复杂的函数，通常我们会把**RNN**的多个层堆叠在一起构建更深的模型。这节视频里我们会学到如何构建这些更深的**RNN**。

一个标准的神经网络，首先是输入$x$，然后堆叠上隐含层，所以这里应该有激活值，比如说第一层是$a^{\left\lbrack 1 \right\rbrack}$，接着堆叠上下一层，激活值$a^{\left\lbrack 2 \right\rbrack}$，可以再加一层$a^{\left\lbrack 3 \right\rbrack}$，然后得到预测值$\hat{y}$。深层的**RNN**网络跟这个有点像，用手画的这个网络（下图编号1所示），然后把它按时间展开就是了，我们看看。

![](https://tva1.sinaimg.cn/large/006y8mN6gy1g98yo5y7gjj30go08lgmq.jpg)

这是我们一直见到的标准的**RNN**（上图编号3所示方框内的**RNN**），只是我把这里的符号稍微改了一下，不再用原来的$a^{<0 >}$表示0时刻的激活值了，而是用$a^{\lbrack 1\rbrack <0>}$来表示第一层（上图编号4所示），所以我们现在用$a^{\lbrack l\rbrack <t>}$来表示第l层的激活值，这个$\<t\>$表示第$t$个时间点，这样就可以表示。第一层第一个时间点的激活值$a^{\lbrack 1\rbrack <1>}$，这（$a^{\lbrack 1\rbrack <2>}$）就是第一层第二个时间点的激活值，$a^{\lbrack 1\rbrack <3>}$和$a^{\lbrack 1\rbrack <4>}$。然后我们把这些（上图编号4方框内所示的部分）堆叠在上面，这就是一个有三个隐层的新的网络。

我们看个具体的例子，看看这个值（$a^{\lbrack 2\rbrack <3>}$，上图编号5所示）是怎么算的。激活值$a^{\lbrack 2\rbrack <3>}$有两个输入，一个是从下面过来的输入（上图编号6所示），还有一个是从左边过来的输入（上图编号7所示），$a^{\lbrack 2\rbrack < 3 >} = g(W_{a}^{\left\lbrack 2 \right\rbrack}\left\lbrack a^{\left\lbrack 2 \right\rbrack < 2 >},a^{\left\lbrack 1 \right\rbrack < 3 >} \right\rbrack + b_{a}^{\left\lbrack 2 \right\rbrack})$，这就是这个激活值的计算方法。参数$W_{a}^{\left\lbrack 2 \right\rbrack}$和$b_{a}^{\left\lbrack 2 \right\rbrack}$在这一层的计算里都一样，相对应地第一层也有自己的参数$W_{a}^{\left\lbrack 1 \right\rbrack}$和$b_{a}^{\left\lbrack 1 \right\rbrack}$。

![](https://tva1.sinaimg.cn/large/006y8mN6gy1g98yo6mumtj30go09gq45.jpg)

对于像左边这样标准的神经网络，你可能见过很深的网络，甚至于100层深，而对于**RNN**来说，有三层就已经不少了。由于时间的维度，**RNN**网络会变得相当大，即使只有很少的几层，很少会看到这种网络堆叠到100层。但有一种会容易见到，就是在每一个上面堆叠循环层，把这里的输出去掉（上图编号1所示），然后换成一些深的层，这些层并不水平连接，只是一个深层的网络，然后用来预测$y^{<1>}$。同样这里（上图编号2所示）也加上一个深层网络，然后预测$y^{<2>}$。这种类型的网络结构用的会稍微多一点，这种结构有三个循环单元，在时间上连接，接着一个网络在后面接一个网络，当然$y^{<3>}$和$y^{<4>}$也一样，这是一个深层网络，但没有水平方向上的连接，所以这种类型的结构我们会见得多一点。通常这些单元（上图编号3所示）没必要非是标准的**RNN**，最简单的**RNN**模型，也可以是**GRU**单元或者**LSTM**单元，并且，你也可以构建深层的双向**RNN**网络。由于深层的**RNN**训练需要很多计算资源，需要很长的时间，尽管看起来没有多少循环层，这个也就是在时间上连接了三个深层的循环层，你看不到多少深层的循环层，不像卷积神经网络一样有大量的隐含层。

这就是深层**RNN**的内容，从基本的**RNN**网络，基本的循环单元到**GRU**，**LSTM**，再到双向**RNN**，还有深层版的模型。这节课后，你已经可以构建很不错的学习序列的模型了。

