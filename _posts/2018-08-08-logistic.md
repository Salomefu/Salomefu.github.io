---
layout: post
title: shallow_network
subtitle: 
date: 2018-08-12
author: Salome
header-img: img/post-bg-2015.jpg
catalog: true
tags:
    - machine learning

---





# 前言

logistic回归模型是对数线性模型，是分类模型。由条件概率分布$P(Y|X)$表示，形式化为**参数化**的logistic分布。

# 模型

logistic回归模型是如下的条件概率分布（模型由概率分布表示，数学模型是什么？我理解是表达*客观输入数据*的一种方式（方程），这种表达方式可以是概率模型也可以是非概率模型，比如常微分方程）




$$
P(Y=1)=\frac{exp(wx+b)}{1+exp(wx+b)}
$$

$$
P(Y=0)=\frac{1}{1+exp(wx+b)}
$$

其中$x\in R^{n}$ 是输入，$Y \in {0,1}$是输出，$w \in R^n$是权值向量。

由图可知，只要$wx+b>0$那么$P(Y=1)$的概率就大于0.5，所以如果以0.5为判断两类的分界值，那么就相当于在判断$wx+b>0$是否成立。

# 策略

模型只是关于输入和输出的映射关系，如何通过大量数据集合估计模型中的参数？这需要我们设计一定的策略，通常是让模型的输出$\hat Y$与实际的Y值去比较进而构造出一个包含参数和所有样本集合的优化函数。

logistic采用最大似然函数法构造优化函数。假设$P(Y=1)=\pi(x)$,那么似然函数构造如下
$$
\prod_{i=1}^M[\pi(x_{i})]^{y_{i}}[1-\pi(x_{i})]^{1-y_{i}}
$$
这个函数说的是我们把数据点放到我们构造的模型中得出预测结果，我们希望出现我们已有样本这个概率越大越好。转换为对数似然函数为
$$
L(w)=\sum_{i=1}^M[y_{i}log(\pi(x_{i}))+(1-y_{i})log(1-\pi(x_{i})]
$$
这样问题就变成了以对数似然函数为目标函数的最优化问题。

# 算法

 以上的最优化问题可以通过梯度下降法来优化。

将最大化似然函数转换为极小化问题，就相当于是在定义所有样本上的代价函数。
$$
J(w)=-\frac{1}{M}\sum_{i=1}^M[y_{i}log(\pi(x_{i}))+(1-y_{i})log(1-\pi(x_{i})]
$$

## 非向量化版本的梯度下降

### 单个样本的梯度下降

假设每个样本只有两个特征$x^{(1)}$和$x^{(2)}$

有如下定义
$$
z = w_{1}x^{(1)}+w_{2}x^{(1)}+b
$$

$$
\hat {y} = a = \sigma(z)
$$

$$
\sigma(z) = \frac{1}{1+e^{-z}}
$$

那么单个样本的损失函数为
$$
L=-ylog(a)-(1-y)log(1-a)
$$
求导
$$
\frac{dL}{da} = -\frac{y}{a}+\frac{1-y}{1-a}
$$

$$
\frac{dL}{dz}=\frac{dL}{da}\frac{da}{dz}=(-\frac{y}{a}+\frac{1-y}{1-a})a(1-a)=a-y
$$

$$
\frac{dL}{dw_{1}}=(a-y)x^{(1)}
$$

$$
\frac{dL}{dw_{2}}=(a-y)x^{(2)}
$$

$$
\frac{dL}{db}=(a-y)
$$

单个样本点的梯度下降更新一次的步骤为
$$
w_{1}=w_{1}-\alpha d{w_{1}}
$$

$$
w_{2}=w_{2}-\alpha d{w_{2}}
$$

$$
b=b-\alpha db
$$

### M个样本的梯度下降

伪代码如下

```python
J=0;dw1=0;dw2=0;db=0;
for i = 1 to M
    z(i) = wx(i)+b;
    a(i) = sigmoid(z(i));
    J += -y(i)log(a(i))-(1-y(i)log(1-a(i));
    dz(i) = a(i)-y(i);
    dw1 += x1(i)dz(i);
    dw2 += x2(i)dz(i);
    db += dz(i);
J/= m;
dw1/= m;
dw2/= m;
db/= m;
w=w-alpha*dw
b=b-alpha*db
```

for循环一次只是遍历所有的样本然后更新一次参数，如果要多次更新，还需要嵌套外层for循环。并且我们这个例子现在只有2个特征，如果我们有M个特征，我们还需要for循环去遍历所有的特征。由于多次for循环效率很低，所以我们需要向量化技术（vectorization）。

## 向量化版本的梯度下降

向量化是非常基础的去处代码中for循环的艺术。

1. 非向量化版本计算z

   ```python
   z=0
   # n是指输入的维度
   for i in range(n):
     z += w[i]*x[i]
     z += b
   ```

   

2. 向量化版本计算z

   ```python
   z=np.dot(w,x)+b
   ```

   

向量化底层利用的是CPU和GPU向量并行化的指令，叫做SMID指令。

### 向量化逻辑回归

将所有样本横向堆叠在一起成为一个$n*m$的矩阵,n代表每个样本的维度，m代表样本个数。w系数矩阵为$1*$n的列向量。b为实数。
$$
X = \begin{bmatrix}
\vdots & \vdots &\vdots \\
x_{1}&x_{2}&x_{3}\\
\vdots & \vdots &\vdots \\
\end{bmatrix}
$$

```python
# 向量化的计算Z.这里省去了遍历所有样本以及所有特征的两重for循环
# 这个表达式利用了broadcasting
# 这里z为1*m的列向量
Z = np.dot(w.T,X)+b
```

### 向量化逻辑回归的梯度

$$
dZ=[dz_{1},dz_{2},...,dz_{m}]\\
A = [a_{1},a_{2},...,a_{m}]\\
Y=[y_{1},y_{2},..y_{m}]\\
dZ = A-Y\\
db=\frac{1}{m}\sum_{i=1}^Mdz_{i}=\frac{1}{m}*np.sum(dZ)\\
dw=\frac{1}{m}*(x_{1}dz_{1}+x_{2}dz_{2}+..+x_{m}dz_{m})=\frac{1}{m}*X*dz^T\\
w = w-\alpha*dw\\
b = b-\alpha*db
$$

## 

# 代码实现

这里以一个识别猫为例

```python
import numpy as np


def sigmoid(z):
    """
    z:可能是标量也可能是array
    """
    return 1.0 / (1.0 + np.exp(-1.0 * z))


def initialize_with_zeros(dim):
    """
    dim:w的维度
    returns:
    w -- (dim,1)
    b -- 0
    """
    w = np.zeros((dim, 1))
    b = 0
    return w, b


def propagate(w, b, X, Y):
    """
    实现损失函数以及它的梯度计算
    w--(dim,1)
    b--scaler
    X--(dim,number of examples)
    Y--(1,number of examples)
    returns:
    cost--负log损失函数
    dw--w的导数
    db--b的导数
    """
    # 样本数量
    m = X.shape[1]
    # 前向传播过程
    A = sigmoid(np.dot(w.T, X) + b)
    cost = (-1.0 / m) * np.sum(Y * np.log(A) + (1 - Y) * np.log(1 - A))
    # 反向传播过程
    dw = (1.0 / m) * np.dot(X, (A - Y).T)
    db = (1.0 / m) * np.sum(A - Y)

    cost = np.squeeze(cost)
    grads = {
        "dw": dw,
        "db": db
    }
    return grads, cost


def optimize(w, b, X, Y, num_iteration, learning_rate, print_cost=False):
    costs = []
    for i in range(num_iteration):
        grads, cost = propagate(w, b, X, Y)
        dw = grads['dw']
        db = grads['db']
        w = w - learning_rate * dw
        b = b - learning_rate * db

        # 每隔100轮记录一下
        if i % 100 == 0:
            costs.append(cost)

        if print_cost and i % 100 == 0:
            print("cost after iteration %i:%f" % (i, cost))
    # 优化后的参数
    params = {
        "w": w,
        "b": b
    }
    # 最后一轮的梯度
    grads = {
        "dw": dw,
        "db": db
    }
    return params, grads, costs


def predict(w, b, X):
    m = X.shape[1]
    Y_prediction = np.zeros((1, m))

    A = sigmoid(np.dot(w.T, X) + b)
    for i in range(A.shape[1]):
        if A[0, i] > 0.5:
            Y_prediction[0, i] = 1
        else:
            Y_prediction[0, i] = 0
    return Y_prediction


def model(X_train, Y_train, X_test, Y_test, num_iterations=2000, learning_rate=0.5, print_cost=False):
    # initialize parameters with zeros (≈ 1 line of code)
    w, b = initialize_with_zeros(X_train.shape[0])

    # Gradient descent (≈ 1 line of code)
    parameters, grads, costs = optimize(w, b, X_train, Y_train, num_iterations, learning_rate, print_cost)

    # Retrieve parameters w and b from dictionary "parameters"
    w = parameters["w"]
    b = parameters["b"]

    # Predict test/train set examples (≈ 2 lines of code)
    Y_prediction_test = predict(w, b, X_test)
    Y_prediction_train = predict(w, b, X_train)

    ### END CODE HERE ###

    # Print train/test Errors
    print("train accuracy: {} %".format(100 - np.mean(np.abs(Y_prediction_train - Y_train)) * 100))
    print("test accuracy: {} %".format(100 - np.mean(np.abs(Y_prediction_test - Y_test)) * 100))

```

