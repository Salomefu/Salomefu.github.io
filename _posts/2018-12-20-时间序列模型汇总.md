---
layout: post
title: 时间序列模型汇总
subtitle: 
date: 2018-12-20
author: Salome
header-img: img/post-bg-2015.jpg
catalog: true
tags:

   - time series
---

[TOC]

# 概况

工作中遇到了很多有关时间序列的问题，发现时间序列预测的性质和传统的机器学习方法还是有很大不同的。调研了一番，关于时间序列预测现在主要有三类方法

- 传统的统计学方法，包括线性模型ARIMA以及金融领域用的比较多的非线性模型门限模型。统计方法的优点是理论成熟，方法论清晰，缺点是易用性不高，涉及到大量的统计检验以及假设。
- 将时间序列预测转换为监督学习问题，用传统的回归方法去做。这个我想大部分人都比较熟悉了。这里的难点就是需要我们自己去构造样本特征，以及做大量的特征工程。
- 由于时序天然的依赖性，可以很好的用RNN及其变体建模，适合有大量的时间序列数据需要同时预测，比如你手上有淘宝几百万个sku的销量数据需要同时预测未来销量。

# 统计学方法

经典的方法如ARIMA是主要运用于单变量、同方差的线性模型。在异方差场合，Engle提出了自回归条件异方差模型（ARCH）模型，随后又有很多人在此基础上拓展成了一系列的广义自回归条件异方差模型。在多变量场合，Granger提出了协整理论，有了协整的概念，在多变量场合变量是平稳的不再是必须条件了。在非线性场合主要有门限自回归模型等等。

## 时间序列的预处理

预处理主要指的是平稳性和纯随机性检验。对于不同性质的时间序列我们要选择不同的模型。

### 平稳性

平稳又分为宽平稳和严平稳。宽平稳有两个重要性质

- 常数均值 $EX_t=\mu$ 
- 常数方差
- 常数协方差-自协方差函数和自相关系数只依赖于时间的平移长度而与时间的起止点无关

#### 时间序列为什么要满足平稳性？

首先，我们要明确当我们拿到了一个观测序列，每个点都是由那个时间步上的随机变量产生的一个样本点。而我们传统的通过样本推断总体性质，都是由N个随机变量，M个样本点。一般的，M越大N越小越好。而由于时间序列的数据结构的特殊性，在任意时刻都只能获得一个观测样本点。由于样本信息太少，如果没有其它辅助信息，通常这种数据结构是没有办法分析的。而序列平稳性概念可以解决这个问题。

#### 平稳性检验

有两种方法，一种是根据时序图和自相关图显示的特征的图检验法；一种是构造检验统计量进行假设检验（单位根检验）的方法。

因为平稳序列的均值和方差为常数，所以平稳序列的时序图会显示该序列始终在一个常数值附近随机波动，并且波动的范围有界。在自相关图中，平稳序列的自相关系数会很快衰减为0，也就是说平稳序列一般具有短期相关性。

以下为图示法以及假设检验的python代码：

```python
# -*- coding:utf-8 -*-
from statsmodels.tsa.stattools import adfuller
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
from statsmodels.graphics.tsaplots import plot_acf, plot_pacf

# 移动平均图
def draw_trend(timeSeries, size):
    f = plt.figure(facecolor='white')
    # 对size个数据进行移动平均
    rol_mean = timeSeries.rolling(window=size).mean()
    # 对size个数据进行加权移动平均
    rol_weighted_mean = pd.ewma(timeSeries, span=size)

    timeSeries.plot(color='blue', label='Original')
    rolmean.plot(color='red', label='Rolling Mean')
    rol_weighted_mean.plot(color='black', label='Weighted Rolling Mean')
    plt.legend(loc='best')
    plt.title('Rolling Mean')
    plt.show()

def draw_ts(timeSeries):
    f = plt.figure(facecolor='white')
    timeSeries.plot(color='blue')
    plt.show()

'''
　　Unit Root Test
   The null hypothesis of the Augmented Dickey-Fuller is that there is a unit
   root, with the alternative that there is no unit root. That is to say the
   bigger the p-value the more reason we assert that there is a unit root
'''
def testStationarity(ts):
    dftest = adfuller(ts)
    # 对上述函数求得的值进行语义描述
    dfoutput = pd.Series(dftest[0:4], index=['Test Statistic','p-value','#Lags Used','Number of Observations Used'])
    for key,value in dftest[4].items():
        dfoutput['Critical Value (%s)'%key] = value
    return dfoutput

# 自相关和偏相关图，默认阶数为31阶
def draw_acf_pacf(ts, lags=31):
    f = plt.figure(facecolor='white')
    ax1 = f.add_subplot(211)
    plot_acf(ts, lags=31, ax=ax1)
    ax2 = f.add_subplot(212)
    plot_pacf(ts, lags=31, ax=ax2)
    plt.show()
```

![](https://tva1.sinaimg.cn/large/006tNbRwgy1gacqvov1nrj30ia0la0vg.jpg)

上图具有上升趋势和周期性，自相关图拖尾呈现长期相关性，明显是非平稳序列。

#### 纯随机性检验

序列平稳性检验之后，如是非平稳序列那么需要进一步的检验、变换之后才能确定合适的拟合模型。对于平稳序列来说，我们还要检验它的纯随机性。并不是所有的平稳序列都值得建模，只有那些序列值之间具有密切的相关关系，历史数据对未来的发展有一定影响的序列， 才值得花时间去挖掘历史数据中的有效信息。如果序列值之间完全没有相关性，这种序列称为纯随机性序列，是没有分析价值的。判断是否纯随机性，就是判断历史上间隔K期的序列值之间是否存在一定程度的相互影响关系。我们分析的目的就是把这种相关信息从序列值中提取出来。一般通过LB统计量进行假设检验判断序列是否纯随机。



## 平稳时间序列分析

ARMA模型是最常用的平稳序列拟合模型。

### AR模型

具有如下结构的模型称为P阶自回归模型，AR(P)
$$
x_t=\phi_0+\phi_1x_{t-1}+...+\phi_px_{t-p}+\epsilon_t
$$
随机干扰序列为0均值白噪声序列

AR模型是用来拟合平稳序列的，但是并不是所有的AR模型都是平稳的。判断平稳性有特征根判别和平稳阈判别。

###### 自相关系数的特性

平稳AR模型的自相关系数有两个显著的特质：一是拖尾性，而是呈指数衰减。

###### 偏自相关系数的特性

偏自相关系数是指去除两个变量中间的影响之后计算的相关系数。

平稳AR模型的偏自相关系数展现截尾性质

### MA模型

具有如下结构的模型称为q阶移动平均模型，MA(q)
$$
x_t=\mu+\epsilon_t-\theta_1\epsilon_{t-1}-...-\theta_q\epsilon_{t-q}
$$
随机干扰序列为0均值白噪声序列

###### 自相关系数的特性

平稳MA模型的自相关系数q阶截尾

###### 偏自相关系数的特性

平稳MA模型的偏自相关系数展现拖尾性质

### ARMA模型

具有如下结构的模型称为自回归移动平均模型，ARMA(p,q)
$$
x_t=\phi_0+\phi_1x_{t-1}+...+\phi_px_{t-p}+\epsilon_t-\theta_1\epsilon_{t-1}-...-\theta_q\epsilon_{t-q}
$$
随机干扰序列为0均值白噪声序列

###### 自相关系数的特性

拖尾

###### 偏自相关系数的特性

拖尾

### 平稳序列建模步骤

1. 根据样本的自相关系数和偏自相关系数定阶
2. 模型参数估计，一般用最大似然法或者最小二乘法。
3. 模型以及参数的显著性检验。模型显著性检验一般是检验残差项是否为白噪声序列（LB检验），参数检验主要是T统计量检验。
4. 模型优化，同一个序列可能可以构造两个模型，每个模型都显著有效。根据AIC以及SBC准则选择模型

- AIC准则 

AIC=-2ln(模型的极大似然函数值)+2(模型中未知参数个数)

AIC最小的模型是最优的模型

- SBC准则

SBC=-2ln(模型的极大似然函数值)+ln(n)(模型中未知参数个数)

## 非平稳序列的确定性分析

非平稳序列的分析方法有确定性分析以及随机性分析两类。

Cramer分解定理：任何一个时间序列都可以分解为两部分的叠加：其中一部分是由多项式决定的确定性成分，另一部分是零均值误差成分。ARMA是在拟合随机序列。

### 确定性因素分解

由确定性因素导致的非平稳，通常都有非常明显的规律，比如有固定的趋势和变化周期。最常用的确定性分析方法是确定性因素分解。

将序列的变化归纳为四大类因素的综合影响：

- 长期趋势
- 循环波动
- 季节性波动
- 随机波动

对这些因素进行组合有加法模型和乘法模型。

statsmodels使用的X-11分解过程，它主要将时序数据分离成长期趋势、季节趋势和随机成分。与其它统计软件一样，statsmodels也支持两类分解模型，加法模型和乘法模型，这里我只实现加法，乘法只需将model的参数设置为"multiplicative"即可。

```python
from statsmodels.tsa.seasonal import seasonal_decompose
decomposition = seasonal_decompose(ts_log, model="additive")

trend = decomposition.trend
seasonal = decomposition.seasonal
residual = decomposition.resid
```

![](https://tva1.sinaimg.cn/large/006tNbRwgy1gacr7f8loyj30ie0cq0vf.jpg)

#### 趋势分析

##### 趋势拟合

分为线性拟合和曲线拟合

##### 平滑法

分为指数平滑法和移动平均法

#### 季节效应分析

$$
x_t=x*S_j+I_j
$$

$S_j$为季节指数，计算方法如下：

1. 计算周期内各期平均值

2. 计算总平均值

3. 用周期平均值除以总平均值

   

#### 综合分析

首先判断趋势和季节是否独立，如果独立用加法模型。否则乘法模型。如果用乘法模型，首先计算季节指数，用原始值除以季节指数消除掉季节影响之后再用线性拟合趋势。

#### Prophet

说到确定性因素分解，就不得不提facebook开源的prophet了[论文地址](https://peerj.com/preprints/3190.pdf)

一般来说，我们用于预测的时间序列有如下特征：

- 对于历史在至少几个月（最好是一年）的每小时、每天或每周的观察
- 强大的多次的「人类规模级」的季节性：每周的一些天和每年的一些时候
- 事先知道的以不定期的间隔发生的重要节假日（如，双十一）
- 合理数量的缺失的观察或大量异常
- 历史趋势改变，比如因为产品发布或记录变化
- 非线性增长曲线的趋势，其中有的趋势达到了自然极限或饱和

Prophet使用的是加性模型（additive model），分别用不同的部分来拟合时间序列不同的趋势，叠加起来则是整个时间序列模型，g(t)是趋势项，表示时间序列在非周期上面的变化趋势。s(t)代表的是季节项或者周期项，h(t)代表的是假日项，剩下的是误差项。Prophet就是通过拟合这几项，然后最后把他们累加起来得到最后的预测结果。
$$
y(t)=g(t)+s(t)+h(t)+\epsilon_t
$$

##### 趋势项

趋势项有两个重要函数，一个是基于分段逻辑回归函数的，另外一个是基于分段线性函数的。

- 逻辑回归函数

$$
f(x)=C/(1+e^{-k(x-m)})
$$

C称为曲线的最大渐进值，k表示曲线的增长率，m表示曲线的中点。除此之外，在现实的时间序列中，三个参数不可能都是常数，而是随着时间变化的函数。所谓的C(Capacity)是需要提前设置好的，曲线的增长率是指在change point的时候的增长率。

- 分段的线性函数

$$
y=kx+b
$$

###### 变点的选择

我们需要给出变点的位置，个数以及增长的变化率。有三个比较重要的指标，那就是

- changepoint_range，

- n_changepoint，

- changepoint_prior_scale。

changepoint_range 指的是百分比，需要在前 changepoint_range 那么长的时间序列中设置变点，在默认的函数中是 changepoint_range = 0.8。n_changepoint 表示变点的个数，在默认的函数中是 n_changepoint = 25。changepoint_prior_scale 表示变点增长率的分布情况，在论文中， ![[公式]](https://www.zhihu.com/equation?tex=+%5Cdelta_%7Bj%7D+%5Csim+Laplace%280%2C%5Ctau%29) ，这里的 ![[公式]](https://www.zhihu.com/equation?tex=%5Ctau) 就是 change_prior_scale。

在整个开源框架里面，在默认的场景下，变点的选择是基于时间序列的前 80% 的历史数据，然后通过等分的方法找到 25 个变点（change points），而变点的增长率是满足 Laplace 分布 ![[公式]](https://www.zhihu.com/equation?tex=%5Cdelta_%7Bj%7D+%5Csim+Laplace+%280%2C0.05%29) 的。因此，当 ![[公式]](https://www.zhihu.com/equation?tex=%5Ctau) 趋近于零的时候， ![[公式]](https://www.zhihu.com/equation?tex=+%5Cdelta_%7Bj%7D) 也是趋向于零的，此时的增长函数将变成全段的逻辑回归函数或者线性函数。这一点从 ![[公式]](https://www.zhihu.com/equation?tex=g%28t%29) 的定义可以轻易地看出。

##### 季节性趋势

在数学分析中，区间内的周期性函数是可以通过正弦和余弦的函数来表示的：假设 ![[公式]](https://www.zhihu.com/equation?tex=+f%28x%29) 是以 ![[公式]](https://www.zhihu.com/equation?tex=2%5Cpi) 为周期的函数，那么它的傅立叶级数就是 ![[公式]](https://www.zhihu.com/equation?tex=a_%7B0%7D+%2B+%5Csum_%7Bn%3D1%7D%5E%7B%5Cinfty%7D%28a_%7Bn%7D%5Ccos%28nx%29+%2B+b_%7Bn%7D%5Csin%28nx%29%29) 。

在论文中，作者使用傅立叶级数来模拟时间序列的周期性。假设 ![[公式]](https://www.zhihu.com/equation?tex=P) 表示时间序列的周期， ![[公式]](https://www.zhihu.com/equation?tex=P+%3D+365.25) 表示以年为周期， ![[公式]](https://www.zhihu.com/equation?tex=P+%3D+7) 表示以周为周期。它的傅立叶级数的形式都是：

![[公式]](https://www.zhihu.com/equation?tex=s%28t%29+%3D+%5Csum_%7Bn%3D1%7D%5E%7BN%7D%5Cbigg%28+a_%7Bn%7D%5Ccos%5Cbigg%28%5Cfrac%7B2%5Cpi+n+t%7D%7BP%7D%5Cbigg%29+%2B+b_%7Bn%7D%5Csin%5Cbigg%28%5Cfrac%7B2%5Cpi+n+t%7D%7BP%7D%5Cbigg%29%5Cbigg%29.)

就作者的经验而言，对于以年为周期的序列（ ![[公式]](https://www.zhihu.com/equation?tex=P+%3D+365.25) ）而言， ![[公式]](https://www.zhihu.com/equation?tex=N+%3D+10) ；对于以周为周期的序列（![[公式]](https://www.zhihu.com/equation?tex=P+%3D+7) ）而言， ![[公式]](https://www.zhihu.com/equation?tex=N+%3D+3) 。这里的参数可以形成列向量：

![[公式]](https://www.zhihu.com/equation?tex=%5Cboldsymbol%7B%5Cbeta%7D+%3D+%28a_%7B1%7D%2Cb_%7B1%7D%2C%5Ccdots%2Ca_%7BN%7D%2Cb_%7BN%7D%29%5E%7BT%7D.)

当 ![[公式]](https://www.zhihu.com/equation?tex=N+%3D+10) 时，

![[公式]](https://www.zhihu.com/equation?tex=X%28t%29+%3D+%5Cbigg%5B%5Ccos%5Cbigg%28%5Cfrac%7B2%5Cpi%281%29t%7D%7B365.25%7D%5Cbigg%29%2C%5Ccdots%2C%5Csin%5Cbigg%28%5Cfrac%7B2%5Cpi%2810%29t%7D%7B365.25%7D%5Cbigg%29%5Cbigg%5D)

当 ![[公式]](https://www.zhihu.com/equation?tex=N+%3D+3) 时，

![[公式]](https://www.zhihu.com/equation?tex=X%28t%29+%3D+%5Cbigg%5B%5Ccos%5Cbigg%28%5Cfrac%7B2%5Cpi%281%29t%7D%7B7%7D%5Cbigg%29%2C%5Ccdots%2C%5Csin%5Cbigg%28%5Cfrac%7B2%5Cpi%283%29t%7D%7B7%7D%5Cbigg%29%5Cbigg%5D.)

因此，时间序列的季节项就是： ![[公式]](https://www.zhihu.com/equation?tex=s%28t%29+%3D+X%28t%29+%5Cboldsymbol%7B%5Cbeta%7D%2C) 而 ![[公式]](https://www.zhihu.com/equation?tex=%5Cboldsymbol%7B%5Cbeta%7D) 的初始化是 ![[公式]](https://www.zhihu.com/equation?tex=%5Cboldsymbol%7B%5Cbeta%7D+%5Csim+Normal%280%2C%5Csigma%5E%7B2%7D%29)。这里的 ![[公式]](https://www.zhihu.com/equation?tex=%5Csigma) 是通过 seasonality_prior_scale 来控制的，也就是说 ![[公式]](https://www.zhihu.com/equation?tex=%5Csigma%3D) seasonality_prior_scale。这个值越大，表示季节的效应越明显；这个值越小，表示季节的效应越不明显。同时，在代码里面，seasonality_mode 也对应着两种模式，分别是加法和乘法，默认是加法的形式。在开源代码中， ![[公式]](https://www.zhihu.com/equation?tex=X%28t%29) 函数是通过 fourier_series 来构建的。

##### 节假日效应（holidays and events）

由于每个节假日对时间序列的影响程度不一样，例如春节，国庆节则是七天的假期，对于劳动节等假期来说则假日较短。因此，不同的节假日可以看成相互独立的模型，并且可以为不同的节假日设置不同的前后窗口值，表示该节假日会影响前后一段时间的时间序列。用数学语言来说，对与第 ![[公式]](https://www.zhihu.com/equation?tex=i) 个节假日来说， ![[公式]](https://www.zhihu.com/equation?tex=D_%7Bi%7D) 表示该节假日的前后一段时间。为了表示节假日效应，我们需要一个相应的指示函数（indicator function），同时需要一个参数 ![[公式]](https://www.zhihu.com/equation?tex=%5Ckappa_%7Bi%7D) 来表示节假日的影响范围。假设我们有 ![[公式]](https://www.zhihu.com/equation?tex=L) 个节假日，那么

![[公式]](https://www.zhihu.com/equation?tex=h%28t%29%3DZ%28t%29+%5Cboldsymbol%7B%5Ckappa%7D%3D%5Csum_%7Bi%3D1%7D%5E%7BL%7D+%5Ckappa_%7Bi%7D%5Ccdot+1_%7B%5C%7Bt%5Cin+D_%7Bi%7D%5C%7D%7D%2C)

其中 ![[公式]](https://www.zhihu.com/equation?tex=Z%28t%29%3D%281_%7B%5C%7Bt%5Cin+D_%7B1%7D%5C%7D%7D%2C%5Ccdots%2C1_%7B%5C%7Bt%5Cin+D_%7BL%7D%5C%7D%7D%29%2C+%5Ctext%7B+%7D+%5Cboldsymbol%7B%5Ckappa%7D%3D%28%5Ckappa_%7B1%7D%2C%5Ccdots%2C%5Ckappa_%7BL%7D%29%5E%7BT%7D.)

其中 ![[公式]](https://www.zhihu.com/equation?tex=%5Cboldsymbol%7B%5Ckappa%7D%5Csim+Normal%280%2Cv%5E%7B2%7D%29) 并且该正态分布是受到 ![[公式]](https://www.zhihu.com/equation?tex=v+%3D) holidays_prior_scale 这个指标影响的。默认值是 10，当值越大时，表示节假日对模型的影响越大；当值越小时，表示节假日对模型的效果越小。用户可以根据自己的情况自行调整。

##### 模型拟合（Model Fitting）

按照以上的解释，我们的时间序列已经可以通过增长项，季节项，节假日项来构建了，i.e.

![[公式]](https://www.zhihu.com/equation?tex=y%28t%29%3Dg%28t%29%2Bs%28t%29%2Bh%28t%29%2B%5Cepsilon)

下一步我们只需要拟合函数就可以了，在 Prophet 里面，作者使用了 pyStan 这个开源工具中的 L-BFGS 方法来进行函数的拟合。

##### Prophet 中可以设置的参数

在 Prophet 中，用户一般可以设置以下四种参数：

1. Capacity：在增量函数是逻辑回归函数的时候，需要设置的容量值。
2. Change Points：可以通过 n_changepoints 和 changepoint_range 来进行等距的变点设置，也可以通过人工设置的方式来指定时间序列的变点。
3. 季节性和节假日：可以根据实际的业务需求来指定相应的节假日。
4. 光滑参数： ![[公式]](https://www.zhihu.com/equation?tex=%5Ctau%3D) changepoint_prior_scale 可以用来控制趋势的灵活度， ![[公式]](https://www.zhihu.com/equation?tex=%5Csigma%3D) seasonality_prior_scale 用来控制季节项的灵活度， ![[公式]](https://www.zhihu.com/equation?tex=v%3D) holidays prior scale 用来控制节假日的灵活度。这些超参数都是要拟合的参数的先验分布的值。

 如果不想设置的话，使用 Prophet 默认的参数即可。

## 非平稳序列的随机分析

非平稳序列的确定性分析原理简单，操作简单易于解释。但有如下问题：

1. 确定性分解只能提取强劲的确定性信息，对随机信息浪费。
2. 无法明确四大因素明确的相互作用关系

前面介绍过的季节指数等方法都是在提取确定性信息，然而都不够充分。差分运算的实质是使用自回归的方式提取确定性信息。1阶差分可以提取线性趋势，二阶查分可以提取曲线趋势，对序列进行周期阶查分可以提取周期信息。因此，差分可以提取序列中的趋势和周期信息，序列呈现典型的随机波动特征。

### 平稳性处理

#### 对数变换

```python
ts_log = np.log(ts)
test_stationarity.draw_ts(ts_log)
```

#### 平滑法

```python
test_stationarity.draw_trend(ts_log, 12)
```

#### 差分

statsmodel中，对差分的支持不是很好，它不支持高阶和多阶差分，最多支持二阶差分。所以需要我们将序列用pandas差分好，之后再还原。

```python
rol_mean = ts_log.rolling(window=12).mean()
rol_mean.dropna(inplace=True)
ts_diff_1 = rol_mean.diff(1)
ts_diff_1.dropna(inplace=True)
test_stationarity.testStationarity(ts_diff_1)
```

根据自相关和偏自相关图定阶。

```python
from statsmodels.tsa.arima_model import ARMA
model = ARMA(ts_diff_1, order=(1, 1)) 
result_arma = model.fit( disp=-1, method='css')
```

```python
predict_ts = result_arma.predict()
# 一阶差分还原
diff_shift_ts = ts_diff_1.shift(1)
diff_recover = predict_ts.add(diff_shift_ts)

# 移动平均还原
rol_sum = ts_log.rolling(window=11).sum()
rol_recover = diff_recover*12 - rol_sum.shift(1)
# 对数还原
log_recover = np.exp(rol_recover)
log_recover.dropna(inplace=True)
```

```python
ts = ts[log_recover.index]  # 过滤没有预测的记录
plt.figure(facecolor='white')
log_recover.plot(color='blue', label='Predict')
ts.plot(color='red', label='Original')
plt.legend(loc='best')
plt.title('RMSE: %.4f'% np.sqrt(sum((log_recover-ts)**2)/ts.size))
plt.show()
```

statsmodels里面的ARIMA模块不支持高阶差分，我们的做法是将差分分离出来，但是这样会多了一步人工还原的操作。基于上述问题，可以将差分过程进行了封装，使序列能按照指定的差分列表依次进行差分，并相应的构造了一个还原的方法，实现差分序列的自动还原。

```python
# 差分操作
def diff_ts(ts, d):
    global shift_ts_list
    #  动态预测第二日的值时所需要的差分序列
    global last_data_shift_list
    shift_ts_list = []
    last_data_shift_list = []
    tmp_ts = ts
    for i in d:
        last_data_shift_list.append(tmp_ts[-i])
        print last_data_shift_list
        shift_ts = tmp_ts.shift(i)
        shift_ts_list.append(shift_ts)
        tmp_ts = tmp_ts - shift_ts
    tmp_ts.dropna(inplace=True)
    return tmp_ts

# 还原操作
def predict_diff_recover(predict_value, d):
    if isinstance(predict_value, float):
        tmp_data = predict_value
        for i in range(len(d)):
            tmp_data = tmp_data + last_data_shift_list[-i-1]
    elif isinstance(predict_value, np.ndarray):
        tmp_data = predict_value[0]
        for i in range(len(d)):
            tmp_data = tmp_data + last_data_shift_list[-i-1]
    else:
        tmp_data = predict_value
        for i in range(len(d)):
            try:
                tmp_data = tmp_data.add(shift_ts_list[-i-1])
            except:
                raise ValueError('What you input is not pd.Series type!')
        tmp_data.dropna(inplace=True)
    return tmp_data
diffed_ts = diff_ts(ts_log, d=[12, 1])
model = arima_model(diffed_ts)
model.certain_model(1, 1)
predict_ts = model.properModel.predict()
diff_recover_ts = predict_diff_recover(predict_ts, d=[12, 1])
log_recover = np.exp(diff_recover_ts)
```

通过BIC准则自动寻找最优的阶数

```python
def proper_model(data_ts, maxLag):
    init_bic = sys.maxint
    init_p = 0
    init_q = 0
    init_properModel = None
    for p in np.arange(maxLag):
        for q in np.arange(maxLag):
            model = ARMA(data_ts, order=(p, q))
            try:
                results_ARMA = model.fit(disp=-1, method='css')
            except:
                continue
            bic = results_ARMA.bic
            if bic < init_bic:
                init_p = p
                init_q = q
                init_properModel = results_ARMA
                init_bic = bic
    return init_bic, init_p, init_q, init_properModel
```



### ARIMA

由于差分运算可以提取趋势和周期信息，许多非平稳的序列经过差分运算后后显示平稳序列的性质。ARIMA模型实质上就是差分运算和ARMA模型的结合。

#### 季节模型

ARIMA可以对具有季节效应的序列建模，可以分为简单季节模型和乘积季节模型。

##### 简单季节模型

周期和趋势以及随机项是加法模型，这时只要通过低阶差分去除趋势，周期阶差分去除周期趋势即可用ARMA建模

##### 乘积季节模型

有些季节性因素无法通过差分操作简单的去除，因为趋势和季节影响不是相互独立的。这时可以用SARIMA模型。

ARIMA(p,d,q)X(P,D,Q)s

前面一部分为非季节性分量，后面为季节性分量，s为周期。

我们将使用一个名为“来自美国夏威夷Mauna Loa天文台的连续空气样本的大气二氧化碳”的数据集，该数据集从1958年3月至2001年12月期间收集了二氧化碳样本

```python
import warnings
import itertools
import pandas as pd
import numpy as np
import statsmodels.api as sm
import matplotlib.pyplot as plt

plt.style.use('fivethirtyeight')

data = sm.datasets.co2.load_pandas()
y = data.data

# 重采样
y = y['co2'].resample('MS').mean()
y = y.fillna(y.bfill())
print(y)
# 可视化 当我们绘制数据时，会出现一些可区分的模式。 时间序列具有明显的季节性格局，总体呈上升趋势。
y.plot(figsize=(15, 6))
plt.show()

# 使用网格搜索来迭代地搜索不同的超参数
p = d = q = range(0, 2)
pdq = list(itertools.product(p, d, q))
seasonal_pdq = [(x[0], x[1], x[2], 12) for x in list(itertools.product(p, d, q))]

print('Examples of parameter combinations for Seasonal ARIMA...')
print('SARIMAX: {} x {}'.format(pdq[1], seasonal_pdq[1]))
print('SARIMAX: {} x {}'.format(pdq[1], seasonal_pdq[2]))
print('SARIMAX: {} x {}'.format(pdq[2], seasonal_pdq[3]))
print('SARIMAX: {} x {}'.format(pdq[2], seasonal_pdq[4]))

warnings.filterwarnings("ignore")  # specify to ignore warning messages

for param in pdq:
    for param_seasonal in seasonal_pdq:
        try:
            mod = sm.tsa.statespace.SARIMAX(y,
                                            order=param,
                                            seasonal_order=param_seasonal,
                                            enforce_stationarity=False,
                                            enforce_invertibility=False)

            results = mod.fit()

            print('ARIMA{}x{}12 - AIC:{}'.format(param, param_seasonal, results.aic))
        except:
            continue

# 拟合
mod = sm.tsa.statespace.SARIMAX(y,
                                order=(1, 1, 1),
                                seasonal_order=(1, 1, 1, 12),
                                enforce_stationarity=False,
                                enforce_invertibility=False)

results = mod.fit()

print(results.summary().tables[1])

# 检验
results.plot_diagnostics(figsize=(15, 12))
plt.show()

# 预测
pred = results.get_prediction(start=pd.to_datetime('1998-01-01'), dynamic=False)
pred_ci = pred.conf_int()

# 绘图
ax = y['1990':].plot(label='observed')
pred.predicted_mean.plot(ax=ax, label='One-step ahead Forecast', alpha=.7)

ax.fill_between(pred_ci.index,
                pred_ci.iloc[:, 0],
                pred_ci.iloc[:, 1], color='k', alpha=.2)

ax.set_xlabel('Date')
ax.set_ylabel('CO2 Levels')
plt.legend()

plt.show()

# 使用RMSE判断
y_forecasted = pred.predicted_mean
y_truth = y['1998-01-01':]
mse = ((y_forecasted - y_truth) ** 2).mean()
print('The Mean Squared Error of our forecasts is {}'.format(round(mse, 2)))

# 生成未来的预测
pred_uc = results.get_forecast(steps=500)
pred_ci = pred_uc.conf_int()
ax = y.plot(label='observed', figsize=(20, 15))
pred_uc.predicted_mean.plot(ax=ax, label='Forecast')
ax.fill_between(pred_ci.index,
                pred_ci.iloc[:, 0],
                pred_ci.iloc[:, 1], color='k', alpha=.25)
ax.set_xlabel('Date')
ax.set_ylabel('CO2 Levels')

plt.legend()
plt.show()

```

#### SARIMAX

X代表外生变量。

#### 残差自回归

首先提取确定性因素，对剩下的随机项用自回归拟合。

# 机器学习方法

ARIMA模型本质上都是线性模型。对于非线性问题，可以用门限回归等模型。另一种思路是将时间序列预测转换为常见的监督学习问题，通过构造特征的方式用常见的回归模型拟合。

常见的一些通用特征如下

- 时间特征

```python
df['dayofmonth'] = df.date.dt.day
df['dayofyear'] = df.date.dt.dayofyear
df['dayofweek'] = df.date.dt.dayofweek
df['month'] = df.date.dt.month
df['year'] = df.date.dt.year
df['weekofyear'] = df.date.dt.weekofyear
df['is_month_start'] = (df.date.dt.is_month_start).astype(int)
df['is_month_end'] = (df.date.dt.is_month_end).astype(int)
```

- 统计特征

```python
def create_sales_agg_monthwise_features(df, gpby_cols, target_col, agg_funcs):
    gpby = df.groupby(gpby_cols)
    newdf = df[gpby_cols].drop_duplicates().reset_index(drop=True)
    for agg_name, agg_func in agg_funcs.items():
        aggdf = gpby[target_col].agg(agg_func).reset_index()
        aggdf.rename(columns={target_col:target_col+'_'+agg_name}, inplace=True)
        newdf = newdf.merge(aggdf, on=gpby_cols, how='left')
    return newdf
```

- 滞后项

```python
def create_sales_lag_feats(df, gpby_cols, target_col, lags):
    gpby = df.groupby(gpby_cols)
    for i in lags:
        df['_'.join([target_col, 'lag', str(i)])] = \
            gpby[target_col].shift(i).values + np.random.normal(scale=1.6, size=(len(df),))
    return df

```

- 移动平均特征

```python
def create_sales_rmean_feats(df, gpby_cols, target_col, windows, min_periods=2, 
                             shift=1, win_type=None):
    gpby = df.groupby(gpby_cols)
    for w in windows:
        df['_'.join([target_col, 'rmean', str(w)])] = \
            gpby[target_col].shift(shift).rolling(window=w, 
                                                  min_periods=min_periods,
                                                  win_type=win_type).mean().values +\
            np.random.normal(scale=1.6, size=(len(df),))
    return df
```

- 移动中位数

```python
def create_sales_rmed_feats(df, gpby_cols, target_col, windows, min_periods=2, 
                            shift=1, win_type=None):
    gpby = df.groupby(gpby_cols)
    for w in windows:
        df['_'.join([target_col, 'rmed', str(w)])] = \
            gpby[target_col].shift(shift).rolling(window=w, 
                                                  min_periods=min_periods,
                                                  win_type=win_type).median().values +\
            np.random.normal(scale=1.6, size=(len(df),))
    return df
```

- 指数权重

```python
def create_sales_ewm_feats(df, gpby_cols, target_col, alpha=[0.9], shift=[1]):
    gpby = df.groupby(gpby_cols)
    for a in alpha:
        for s in shift:
            df['_'.join([target_col, 'lag', str(s), 'ewm', str(a)])] = \
                gpby[target_col].shift(s).ewm(alpha=a).mean().values
    return df
```

- 类别型变量 one hot-encoding

```python
def one_hot_encoder(df, ohe_cols=['store','item','dayofmonth','dayofweek','month','weekofyear']):
    '''
    One-Hot Encoder function
    '''
    print('Creating OHE features..\nOld df shape:{}'.format(df.shape))
    df = pd.get_dummies(df, columns=ohe_cols)
    print('New df shape:{}'.format(df.shape))
    return df
```



# 深度学习方法

时间序列天然的时间依赖适合用RNN以及变体LSTM建模，Amazon提出的DeepAR，本质上就是LSTM cell加上encoder-decoder结构。这种方法适合有几十万个以上大量的时间序列需要同时预测的场景。

要使用DeepAR,只能用亚马逊的Sagemaker平台云部署了。