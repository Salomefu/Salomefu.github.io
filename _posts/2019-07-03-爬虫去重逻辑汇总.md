---
layout: post
title: 爬虫去重逻辑汇总
subtitle: 
date: 2019-07-03
author: Salome
header-img: img/post-bg-2015.jpg
catalog: true
tags:

   - 爬虫
---

# 爬虫去重

## 业务场景

1. 防止发生重复的请求
2. 防止重复数据入库

## 实现去重的基本原理

根据给定的**判断依据**和给定的**去重容器**，将原始数据或者原始数据的特征值逐个判断去重容器是否含有该值，如果不是重复的就加入到去重容器中，并且将原始数据加入到结果。

判断依据：如何规定两个数据是重复的，基于原始数据也可能是**原始数据的特征值**, 判断依据根据业务确定，不一定两个数据==才算是重复。

```python
# 判断依据不一定是 ==
Class Test():
  self.v = v
t1 = Test(1)
t2 = test(2)
t3 = Test(2)
# 规定类型相同就是重复
data = [t1,t2,t3]
result = [set(i.__class__) for i in data] 
# 规定v相同是重复
result = [set(i.v) for i in data]
```

去重容器 存储判断依据

```python
# 规定100和“100”相等 假设数据是以流的形式过来
data = [123,'123']
result = []
# 去重容器 存储判断依据 非常重要
sign = []
new_data = None
if str(new_data) not in sign:
  result.append(new_data)
  sign.append(str(new_data))
```

### 原始数据和原始数据的特征值

为什么需要计算原始数据特征值？

效率问题。如果原始数据很大，判断计算耗时。所以先计算原始数据的唯一特征值（比如md5等），再去做判断。

### 临时去重容器和持久化去重容器

临时：list、set等    优点是使用简单，缺点是无法共享也无法持久化

持久化：mysql、redis等，优缺点与以上相反

## 信息摘要算法计算原始数据特征值

将任意长度的文本通过一个算法（md5 128位 、SHA1 160位）得到一个固定长度的文本，只要原文本不同那么计算得到的结果必然不同。利用信息摘要算法可以降低去重容器的存储空间使用率，并提高判断速度。

```python
import hashlib
import six
class BaseFilter(object):
    def __init__(self,
                 hash_func_name='md5',
                 redis_host="localhost",
                 redis_port=6379,
                 redis_db=0,
                 redis_key="filter",
                 mysql_url=None,
                 mysql_table_name="Filter"
                 ):

        self.redis_host = redis_host
        self.redis_port = redis_port
        self.redis_db = redis_db
        self.redis_key = redis_key
        self.mysql_url = mysql_url
        self.mysql_table_name = mysql_table_name

        self.hash_func = getattr(hashlib, hash_func_name)
        self.storage = self._get_storage()

    def _get_storage(self):
        pass

    def _safe_data(self, data):
        """
        python 2 str === python3 bytes
        python2 unicode === python3 str
        :param data:
        :return: 二进制类型的字符串数据
        """
        if six.PY3:
            if isinstance(data, bytes):
                return data
            elif isinstance(data, str):
                return data.encode()
            else:
                raise Exception("please input a str")

        else:
            if isinstance(data, str):
                return data
            elif isinstance(data, unicode):
                return data.encode()
            else:
                raise Exception("please input a str")

    def _get_hash_value(self, data):
        """

        :param data: 给定的原始数据
        :return: hash值
        """
        hash_obj = self.hash_func()
        hash_obj.update(self._safe_data(data))
        hash_value = hash_obj.hexdigest()
        return hash_value

    def save(self, data):
        hash_value = self._get_hash_value(data)
        self._save(hash_value)

    def _save(self, hash_value):
        """
        交给子类继承
        :param hash_value:
        :return:
        """
        pass

    def is_exists(self, data):
        hash_value = self._get_hash_value(data)
        return self._is_exists(hash_value)

    def _is_exists(self, hash_value):
        pass
      
```

```python
# 临时容器
from .base import BaseFilter


class MemoryFilter(BaseFilter):
    def _get_storage(self):
        return set()

    def _save(self, hash_value):
        return self.storage.add(hash_value)

    def _is_exists(self, hash_value):
        if hash_value in self.storage:
            return True
        return False

```

```python
# mysql
from sqlalchemy import create_engine, Column, Integer, String
from sqlalchemy.orm import sessionmaker
from sqlalchemy.ext.declarative import declarative_base
from .base import BaseFilter

# pymysql+sql,mysqldb+sql 都是长连接，长时间连接可能断掉；并且是基于原始sql语句的连接
# sqlalchemy 使用orm则不会有这些问题

Base = declarative_base()


class MysqlFilter(BaseFilter):

    # 需求 可以动态更改类名

    def __init__(self, *args, **kwargs):
        # version1
        # class Filter(Base):
        #     __tablename__ = kwargs["mysql_table_name"]
        #     id = Column(Integer, primary_key=True)
        #     hash_value = Column(String(40), index=True, unique=True)
        #
        # self.table = Filter

        # version2
        self.table = type(kwargs["mysql_table_name"], (Base,), dict(
            __tablename__=kwargs["mysql_table_name"],
            id=Column(Integer, primary_key=True),
            hash_value=Column(String(40), index=True, unique=True)
        ))

        BaseFilter.__init__(self, *args, **kwargs)

    def _get_storage(self):
        engine = create_engine(self.mysql_url)
        Base.metedata.create_all(engine)  # 创建表
        Session = sessionmaker(engine)
        return Session

    def _save(self, hash_value):
        session = self.storage()
        filter = self.table(hash_value=hash_value)
        session.add(filter)
        session.commit()
        session.close()

    def _is_exists(self, hash_value):
        session = self.storage()
        ret = session.query(self.table).filter_by(hash_value=hash_value).first()
        session.close()
        if ret is None:
            return False
        return True

```

```python
# redis
from .base import BaseFilter
import redis

class RedisFilter(BaseFilter):
    def _get_storage(self):
        pool = redis.ConnectionPool(self.redis_host,self.redis_port,self.redis_db)
        client = redis.StrictRedis(connection_pool=pool)
        return client

    def _save(self, hash_value):
        return self.storage.sadd(self.redis_key,hash_value)

    def _is_exists(self, hash_value):
        return self.storage.sismember(self.redis_key,hash_value)

```

## simhash算法计算原始数据特征值

simhash算法是局部敏感哈希算法，能实现相似文本内容的去重。通过两者的simhash值的二进制的差异来表示原始文本内容的差异，差异个数被称为海明距离。

1.  simhash对长文本比较适用，短文本偏差较大。
2. 在Google论文给出的数据中，64位simhash值在海明距离为3可以认为两篇文章是相似的。

## 布隆过滤器

通过多个hash算法得到数据的hash值，根据hash表长度求余得到offset，将hash表相应位置改为1.对一个新来的数，以同样方式求得多个offset，如果所有offset都是1那么就说明这个数是重复的（有误判的可能）。降低误判的方式有增多hash算法数目或者增大hash表长度。

```python
import hashlib

import redis as redis
import six
import redis


class MultipleHash(object):
    def __init__(self, salts, hash_func_name='md5'):
        # 加盐的操作类似于用多个hash算法
        self.hash_func = getattr(hashlib, hash_func_name)
        self.salts = salts

    def _safe_data(self, data):
        """
        python 2 str === python3 bytes
        python2 unicode === python3 str
        :param data:
        :return: 二进制类型的字符串数据
        """
        if six.PY3:
            if isinstance(data, bytes):
                return data
            elif isinstance(data, str):
                return data.encode()
            else:
                raise Exception("please input a str")

        else:
            if isinstance(data, str):
                return data
            elif isinstance(data, unicode):
                return data.encode()
            else:
                raise Exception("please input a str")

    def get_hash_values(self, data):
        hash_values = []
        for i in self.salts:
            hash_obj = self.hash_func
            hash_obj.update(self._safe_data(data))
            hash_obj.update(self._safe_data(i))
            ret = hash_obj.hexdigest()
            hash_values.append(int(ret, 16))
        return hash_values


class BloomFilter(object):
    def __init__(self,
                 salts,
                 redis_host="localhost",
                 redis_port=6379,
                 redis_db=0,
                 redis_key="filter",
                 ):
        self.redis_host = redis_host
        self.redis_port = redis_port
        self.redis_db = redis_db
        self.redis_key = redis_key
        self.client = self._get_redis_client()
        self.multiple_hash = MultipleHash(salts)

    def _get_redis_client(self):
        pool = redis.ConnectionPool(self.redis_host, self.redis_port, self.redis_db)
        client = redis.StrictRedis(connection_pool=pool)
        return client

    def save(self, hash_value):
        hash_values = self.multiple_hash.get_hash_values(data)
        for hash_value in hash_values:
            offset = self._get_offset(hash_value)
            self.client.setbit(self.redis_key, offset, 1)

    def _get_offset(self, hash_value):
        # 2**8 = 256m 
        return hash_value % 2 ** 8 * 2 ** 20 * 2 * 3

    def is_exists(self, data):
        hash_values = self.multiple_hash.get_hash_values(data)
        for hash_value in hash_values:
            offset = self._get_offset(hash_value)
            v = self.client.getbit(self.redis_key, offset)
            if 0 == v:
                return False
        return True
if __name__ == '__main__':
    data = ['ahau', 'ahau', 'ahz']
    bm = BloomFilter(salts=["1", "2"], redis_host="localhost")
    for d in data:
        if not bm.is_exists(d):
            bm.save(d)
        else:
            print("发现重复数据")
```

